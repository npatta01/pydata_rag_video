{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4eab10f-af05-453a-b335-a5259f3f4b1d",
   "metadata": {},
   "source": [
    "# Notebook : Why Rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dae9c54-fc1a-476c-ba06-fd0b3ef3cff4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e4a1d-97ea-4913-a562-8a0a88b2715b",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this notebook, we will motivate Retrieval Augmented Generation (RAG) .\n",
    "\n",
    "We will see why we can't use Large Language Model directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1fbb5-5111-4dfe-8ab6-c55db01673be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2fd037-bdbc-4b98-815c-e650930a43d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0bd6025-261b-4735-8c0e-68f7f23c3fd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb2c26-9588-4c17-92f7-e86a34287bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f16f4c-bf75-40d2-9c85-c6e99b1fc56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import rich\n",
    "import logging\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from huggingface_hub import InferenceClient\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b3d6d8-0786-4e50-b879-ed76b81dd650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#logging.basicConfig(level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0dff8e-e2d4-4874-9529-05404682dd75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\"../env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32606a-c066-4f71-bf4b-18c1ba9db121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08eae1-d3e9-4ee2-8d29-2b50d595fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e03fdea-fd48-444e-96a9-51fd0e8db5da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0713199-2b56-4cbf-95fc-5f15c0722227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620eb07-8ec4-4f64-84db-f4cc14082f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60b96e58-e4fc-4f03-9107-776d189ffe67",
   "metadata": {},
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04fc47-e448-4ad6-9e22-46cac490f5cc",
   "metadata": {},
   "source": [
    "There are atleast two problems with using Large Language Model \n",
    "\n",
    "- Knoweldge Cutoff\n",
    "- Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b635b-1dcb-45fb-b321-10fd46c086c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8f41792-ad94-487b-8e17-0a0f465581b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Knoweldge Cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be027950-9c80-4a6e-a293-59b0359c2451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850da7c3-80fe-4b7d-87c7-ff5950cbe48f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\",temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e61e4e8-0646-45fb-8593-2d1a3886b808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response(query:str):\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=query),\n",
    "    ]\n",
    "    \n",
    "    resp = llm.chat(messages)\n",
    "\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fff0b3-1bcb-45e8-8ad8-adfbc2edd5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cab561-8afe-4069-868f-bfc3b94fe7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5e41e1-cc61-400f-b443-5d220e36e97d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query=\"what is different about Llama3.2 than Llama2 ?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfc54077-17a9-48c7-a87c-e6ed0df6e46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = get_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a84f78-1244-47d3-b92f-a0c3931a564e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">As of my last update in October <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>, specific details about Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span> compared to Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> were not available. \n",
       "However, in general, updates from one version of a model to another typically include improvements in several \n",
       "areas:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Model Architecture**: Newer versions may incorporate changes in the underlying architecture that enhance \n",
       "performance, efficiency, or scalability.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Training Data**: Updates often involve training on larger and more diverse datasets, which can improve the \n",
       "model's understanding and generation of language.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Fine-tuning and Specialization**: Newer models may include better fine-tuning techniques or be specialized for\n",
       "certain tasks, leading to improved performance in specific applications.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Performance Metrics**: Improvements in metrics such as accuracy, coherence, and relevance of generated text \n",
       "are common in newer versions.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. **Robustness and Safety**: Enhancements in handling biases, reducing harmful outputs, and improving the overall \n",
       "safety of the model are often prioritized in newer iterations.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. **User Feedback Integration**: Newer models may incorporate feedback from users of previous versions to address \n",
       "shortcomings and improve user experience.\n",
       "\n",
       "For the most accurate and detailed information, I recommend checking the official release notes or documentation \n",
       "from the developers of the Llama models.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "As of my last update in October \u001b[1;36m2023\u001b[0m, specific details about Llama \u001b[1;36m3.2\u001b[0m compared to Llama \u001b[1;36m2\u001b[0m were not available. \n",
       "However, in general, updates from one version of a model to another typically include improvements in several \n",
       "areas:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Model Architecture**: Newer versions may incorporate changes in the underlying architecture that enhance \n",
       "performance, efficiency, or scalability.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Training Data**: Updates often involve training on larger and more diverse datasets, which can improve the \n",
       "model's understanding and generation of language.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Fine-tuning and Specialization**: Newer models may include better fine-tuning techniques or be specialized for\n",
       "certain tasks, leading to improved performance in specific applications.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. **Performance Metrics**: Improvements in metrics such as accuracy, coherence, and relevance of generated text \n",
       "are common in newer versions.\n",
       "\n",
       "\u001b[1;36m5\u001b[0m. **Robustness and Safety**: Enhancements in handling biases, reducing harmful outputs, and improving the overall \n",
       "safety of the model are often prioritized in newer iterations.\n",
       "\n",
       "\u001b[1;36m6\u001b[0m. **User Feedback Integration**: Newer models may incorporate feedback from users of previous versions to address \n",
       "shortcomings and improve user experience.\n",
       "\n",
       "For the most accurate and detailed information, I recommend checking the official release notes or documentation \n",
       "from the developers of the Llama models.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39efe358-f789-4447-a8db-06a3904a3f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=\"As of my last update in October 2023, specific details about Llama 3.2 compared to Llama 2 were not available. However, in general, updates from one version of a model to another typically include improvements in several areas:\\n\\n1. **Model Architecture**: Newer versions may incorporate changes in the underlying architecture that enhance performance, efficiency, or scalability.\\n\\n2. **Training Data**: Updates often involve training on larger and more diverse datasets, which can improve the model's understanding and generation of language.\\n\\n3. **Fine-tuning and Specialization**: Newer models may include better fine-tuning techniques or be specialized for certain tasks, leading to improved performance in specific applications.\\n\\n4. **Performance Metrics**: Improvements in metrics such as accuracy, coherence, and relevance of generated text are common in newer versions.\\n\\n5. **Robustness and Safety**: Enhancements in handling biases, reducing harmful outputs, and improving the overall safety of the model are often prioritized in newer iterations.\\n\\n6. **User Feedback Integration**: Newer models may incorporate feedback from users of previous versions to address shortcomings and improve user experience.\\n\\nFor the most accurate and detailed information, I recommend checking the official release notes or documentation from the developers of the Llama models.\", additional_kwargs={}), raw=ChatCompletion(id='chatcmpl-APXFoifUTQEFmp91ZvnUc1ziIYXdR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As of my last update in October 2023, specific details about Llama 3.2 compared to Llama 2 were not available. However, in general, updates from one version of a model to another typically include improvements in several areas:\\n\\n1. **Model Architecture**: Newer versions may incorporate changes in the underlying architecture that enhance performance, efficiency, or scalability.\\n\\n2. **Training Data**: Updates often involve training on larger and more diverse datasets, which can improve the model's understanding and generation of language.\\n\\n3. **Fine-tuning and Specialization**: Newer models may include better fine-tuning techniques or be specialized for certain tasks, leading to improved performance in specific applications.\\n\\n4. **Performance Metrics**: Improvements in metrics such as accuracy, coherence, and relevance of generated text are common in newer versions.\\n\\n5. **Robustness and Safety**: Enhancements in handling biases, reducing harmful outputs, and improving the overall safety of the model are often prioritized in newer iterations.\\n\\n6. **User Feedback Integration**: Newer models may incorporate feedback from users of previous versions to address shortcomings and improve user experience.\\n\\nFor the most accurate and detailed information, I recommend checking the official release notes or documentation from the developers of the Llama models.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730649392, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0ba0d124f1', usage=CompletionUsage(completion_tokens=258, prompt_tokens=21, total_tokens=279, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))), delta=None, logprobs=None, additional_kwargs={'prompt_tokens': 21, 'completion_tokens': 258, 'total_tokens': 279})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea96bd88-d198-46e6-b6d9-730d7a7c3d92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">MessageRole.ASSISTANT:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"As of my last update in October 2023, specific details about Llama 3.2 compared to Llama 2 were </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">not available. However, in general, updates from one version of a model to another typically include improvements </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in several areas:\\n\\n1. **Model Architecture**: Newer versions may incorporate changes in the underlying </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">architecture that enhance performance, efficiency, or scalability.\\n\\n2. **Training Data**: Updates often involve </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">training on larger and more diverse datasets, which can improve the model's understanding and generation of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language.\\n\\n3. **Fine-tuning and Specialization**: Newer models may include better fine-tuning techniques or be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specialized for certain tasks, leading to improved performance in specific applications.\\n\\n4. **Performance </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Metrics**: Improvements in metrics such as accuracy, coherence, and relevance of generated text are common in newer</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">versions.\\n\\n5. **Robustness and Safety**: Enhancements in handling biases, reducing harmful outputs, and improving</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the overall safety of the model are often prioritized in newer iterations.\\n\\n6. **User Feedback Integration**: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Newer models may incorporate feedback from users of previous versions to address shortcomings and improve user </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experience.\\n\\nFor the most accurate and detailed information, I recommend checking the official release notes or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentation from the developers of the Llama models.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">raw</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-APXFoifUTQEFmp91ZvnUc1ziIYXdR'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"As of my last update in October 2023, specific details about Llama 3.2 compared to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama 2 were not available. However, in general, updates from one version of a model to another typically include </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">improvements in several areas:\\n\\n1. **Model Architecture**: Newer versions may incorporate changes in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">underlying architecture that enhance performance, efficiency, or scalability.\\n\\n2. **Training Data**: Updates </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">often involve training on larger and more diverse datasets, which can improve the model's understanding and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generation of language.\\n\\n3. **Fine-tuning and Specialization**: Newer models may include better fine-tuning </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">techniques or be specialized for certain tasks, leading to improved performance in specific applications.\\n\\n4. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Performance Metrics**: Improvements in metrics such as accuracy, coherence, and relevance of generated text are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">common in newer versions.\\n\\n5. **Robustness and Safety**: Enhancements in handling biases, reducing harmful </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">outputs, and improving the overall safety of the model are often prioritized in newer iterations.\\n\\n6. **User </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Feedback Integration**: Newer models may incorporate feedback from users of previous versions to address </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">shortcomings and improve user experience.\\n\\nFor the most accurate and detailed information, I recommend checking </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the official release notes or documentation from the developers of the Llama models.\"</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">audio</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1730649392</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_0ba0d124f1'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">258</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">279</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reasoning_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">cached_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">delta</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">258</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">279</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmessage\u001b[0m=\u001b[1;35mChatMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mrole\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mMessageRole.ASSISTANT:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'assistant'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m\"As\u001b[0m\u001b[32m of my last update in October 2023, specific details about Llama 3.2 compared to Llama 2 were \u001b[0m\n",
       "\u001b[32mnot available. However, in general, updates from one version of a model to another typically include improvements \u001b[0m\n",
       "\u001b[32min several areas:\\n\\n1. **Model Architecture**: Newer versions may incorporate changes in the underlying \u001b[0m\n",
       "\u001b[32marchitecture that enhance performance, efficiency, or scalability.\\n\\n2. **Training Data**: Updates often involve \u001b[0m\n",
       "\u001b[32mtraining on larger and more diverse datasets, which can improve the model's understanding and generation of \u001b[0m\n",
       "\u001b[32mlanguage.\\n\\n3. **Fine-tuning and Specialization**: Newer models may include better fine-tuning techniques or be \u001b[0m\n",
       "\u001b[32mspecialized for certain tasks, leading to improved performance in specific applications.\\n\\n4. **Performance \u001b[0m\n",
       "\u001b[32mMetrics**: Improvements in metrics such as accuracy, coherence, and relevance of generated text are common in newer\u001b[0m\n",
       "\u001b[32mversions.\\n\\n5. **Robustness and Safety**: Enhancements in handling biases, reducing harmful outputs, and improving\u001b[0m\n",
       "\u001b[32mthe overall safety of the model are often prioritized in newer iterations.\\n\\n6. **User Feedback Integration**: \u001b[0m\n",
       "\u001b[32mNewer models may incorporate feedback from users of previous versions to address shortcomings and improve user \u001b[0m\n",
       "\u001b[32mexperience.\\n\\nFor the most accurate and detailed information, I recommend checking the official release notes or \u001b[0m\n",
       "\u001b[32mdocumentation from the developers of the Llama models.\"\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mraw\u001b[0m=\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-APXFoifUTQEFmp91ZvnUc1ziIYXdR'\u001b[0m,\n",
       "        \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "                \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mcontent\u001b[0m=\u001b[32m\"As\u001b[0m\u001b[32m of my last update in October 2023, specific details about Llama 3.2 compared to \u001b[0m\n",
       "\u001b[32mLlama 2 were not available. However, in general, updates from one version of a model to another typically include \u001b[0m\n",
       "\u001b[32mimprovements in several areas:\\n\\n1. **Model Architecture**: Newer versions may incorporate changes in the \u001b[0m\n",
       "\u001b[32munderlying architecture that enhance performance, efficiency, or scalability.\\n\\n2. **Training Data**: Updates \u001b[0m\n",
       "\u001b[32moften involve training on larger and more diverse datasets, which can improve the model's understanding and \u001b[0m\n",
       "\u001b[32mgeneration of language.\\n\\n3. **Fine-tuning and Specialization**: Newer models may include better fine-tuning \u001b[0m\n",
       "\u001b[32mtechniques or be specialized for certain tasks, leading to improved performance in specific applications.\\n\\n4. \u001b[0m\n",
       "\u001b[32m**Performance Metrics**: Improvements in metrics such as accuracy, coherence, and relevance of generated text are \u001b[0m\n",
       "\u001b[32mcommon in newer versions.\\n\\n5. **Robustness and Safety**: Enhancements in handling biases, reducing harmful \u001b[0m\n",
       "\u001b[32moutputs, and improving the overall safety of the model are often prioritized in newer iterations.\\n\\n6. **User \u001b[0m\n",
       "\u001b[32mFeedback Integration**: Newer models may incorporate feedback from users of previous versions to address \u001b[0m\n",
       "\u001b[32mshortcomings and improve user experience.\\n\\nFor the most accurate and detailed information, I recommend checking \u001b[0m\n",
       "\u001b[32mthe official release notes or documentation from the developers of the Llama models.\"\u001b[0m,\n",
       "                    \u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                    \u001b[33maudio\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33mcreated\u001b[0m=\u001b[1;36m1730649392\u001b[0m,\n",
       "        \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "        \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "        \u001b[33mservice_tier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_0ba0d124f1'\u001b[0m,\n",
       "        \u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m258\u001b[0m,\n",
       "            \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m21\u001b[0m,\n",
       "            \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m279\u001b[0m,\n",
       "            \u001b[33mcompletion_tokens_details\u001b[0m=\u001b[1;35mCompletionTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33maudio_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mreasoning_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mprompt_tokens_details\u001b[0m=\u001b[1;35mPromptTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33maudio_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mcached_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mdelta\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m, \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m258\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m279\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b528f-ebc1-44e1-a2e5-75c9de362c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b0856d-6a9c-4c58-aa5e-282831744fde",
   "metadata": {},
   "source": [
    "note the model is aliased to `model='gpt-4o-mini-2024-07-18'`.\n",
    "\n",
    "So, the model's training data wont have any information after then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da065824-a2fd-4e0c-8484-9521e651798a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fbbf77-64f2-4931-928a-5477aa8a692b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1040e02-ac2f-46f3-a5c0-a4f3b42cb7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d5940d-0c4b-4d55-8759-22e8ae0f8de4",
   "metadata": {},
   "source": [
    "## Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6948c2-186d-4e9c-a447-dc37d50a76a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Snapshot of Hallucination](../images/llama_2__hallucination.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740a1a3-e484-47a0-8841-c81cdb2b3999",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the above video, we ask about non existant Llama model.\n",
    "\n",
    "We also give it a link to a study music youtube video.\n",
    "\n",
    "Note: we are also using an \"older\" version of Llama model to make the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d96fc7-2c75-462f-be93-0589c351b97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a25d23-7e13-4801-b9cc-91409c0dab70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003255e-0137-42bd-bfaf-80d8bde17dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = InferenceClient(api_key=os.environ['HF_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301ed5c-5bcb-469d-b1d3-64ad872cec4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "What did Andrej Karpathy say about Meta's Llama 5.9 in the below youtube talk\n",
    "\n",
    "https://www.youtube.com/watch?v=n61ULEU7CO0&ab_channel=LofiGirl\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d5119-bff3-4ff2-93c1-6ae4a6d113ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": message },\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\", \n",
    "    messages=messages, \n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183225be-de1b-4c1b-8e6d-a37b6e22db93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9a249-66ed-4936-890e-6850a631684a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rich.print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8197d2-3326-4857-87a3-5e22d07e82b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6b42b54-f7f4-4689-ad21-108e552ec308",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798509d-a073-42fe-9315-2b873a9f1d67",
   "metadata": {},
   "source": [
    "We explored at least two problems with directly using LLMs directly. \n",
    "\n",
    "- Knowledge Cuttoff  \n",
    "- Hallucination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb31ae-4e15-4c8a-90c8-d754ec50eccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
