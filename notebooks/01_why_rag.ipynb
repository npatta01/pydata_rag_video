{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4eab10f-af05-453a-b335-a5259f3f4b1d",
   "metadata": {},
   "source": [
    "# Notebook : Why Rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dae9c54-fc1a-476c-ba06-fd0b3ef3cff4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e4a1d-97ea-4913-a562-8a0a88b2715b",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this notebook, we will motivate Retrieval Augmented Generation (RAG) .\n",
    "\n",
    "We will see why we can't use Large Language Model directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1fbb5-5111-4dfe-8ab6-c55db01673be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2fd037-bdbc-4b98-815c-e650930a43d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0bd6025-261b-4735-8c0e-68f7f23c3fd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb2c26-9588-4c17-92f7-e86a34287bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f16f4c-bf75-40d2-9c85-c6e99b1fc56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import rich\n",
    "import logging\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from huggingface_hub import InferenceClient\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b3d6d8-0786-4e50-b879-ed76b81dd650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#logging.basicConfig(level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0dff8e-e2d4-4874-9529-05404682dd75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\"../env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32606a-c066-4f71-bf4b-18c1ba9db121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08eae1-d3e9-4ee2-8d29-2b50d595fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e03fdea-fd48-444e-96a9-51fd0e8db5da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0713199-2b56-4cbf-95fc-5f15c0722227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620eb07-8ec4-4f64-84db-f4cc14082f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60b96e58-e4fc-4f03-9107-776d189ffe67",
   "metadata": {},
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04fc47-e448-4ad6-9e22-46cac490f5cc",
   "metadata": {},
   "source": [
    "There are atleast two problems with using Large Language Model \n",
    "\n",
    "- Knoweldge Cutoff\n",
    "- Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b635b-1dcb-45fb-b321-10fd46c086c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8f41792-ad94-487b-8e17-0a0f465581b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Knoweldge Cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be027950-9c80-4a6e-a293-59b0359c2451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850da7c3-80fe-4b7d-87c7-ff5950cbe48f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\",temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e61e4e8-0646-45fb-8593-2d1a3886b808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response(query:str):\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=query),\n",
    "    ]\n",
    "    \n",
    "    resp = llm.chat(messages)\n",
    "\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fff0b3-1bcb-45e8-8ad8-adfbc2edd5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cab561-8afe-4069-868f-bfc3b94fe7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5e41e1-cc61-400f-b443-5d220e36e97d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query=\"what is different about Llama3.2 than Llama2 ?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc54077-17a9-48c7-a87c-e6ed0df6e46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = get_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a84f78-1244-47d3-b92f-a0c3931a564e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">As of my last update in October <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>, specific details about Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span> compared to Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> have not been officially\n",
       "released. However, in general, when comparing iterations of AI models like Llama, you can expect improvements in \n",
       "several areas:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Model Architecture**: Newer versions often incorporate advancements in model architecture that enhance \n",
       "performance, efficiency, and scalability.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Training Data**: Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span> may be trained on a larger and more diverse dataset, which can improve its \n",
       "understanding of language and context.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Performance**: Improvements in natural language understanding, generation quality, and response coherence are \n",
       "common in newer models.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Fine-tuning and Adaptability**: Enhanced capabilities for fine-tuning on specific tasks or domains may be \n",
       "introduced, allowing for better customization.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. **Safety and Ethical Considerations**: Newer models often include better mechanisms for handling harmful \n",
       "content, bias reduction, and overall ethical AI use.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. **Efficiency**: Optimizations that reduce computational requirements while maintaining or improving performance \n",
       "are often a focus in newer versions.\n",
       "\n",
       "For the most accurate and detailed information, I recommend checking the official announcements or documentation \n",
       "from the developers of the Llama models.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "As of my last update in October \u001b[1;36m2023\u001b[0m, specific details about Llama \u001b[1;36m3.2\u001b[0m compared to Llama \u001b[1;36m2\u001b[0m have not been officially\n",
       "released. However, in general, when comparing iterations of AI models like Llama, you can expect improvements in \n",
       "several areas:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Model Architecture**: Newer versions often incorporate advancements in model architecture that enhance \n",
       "performance, efficiency, and scalability.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Training Data**: Llama \u001b[1;36m3.2\u001b[0m may be trained on a larger and more diverse dataset, which can improve its \n",
       "understanding of language and context.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Performance**: Improvements in natural language understanding, generation quality, and response coherence are \n",
       "common in newer models.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. **Fine-tuning and Adaptability**: Enhanced capabilities for fine-tuning on specific tasks or domains may be \n",
       "introduced, allowing for better customization.\n",
       "\n",
       "\u001b[1;36m5\u001b[0m. **Safety and Ethical Considerations**: Newer models often include better mechanisms for handling harmful \n",
       "content, bias reduction, and overall ethical AI use.\n",
       "\n",
       "\u001b[1;36m6\u001b[0m. **Efficiency**: Optimizations that reduce computational requirements while maintaining or improving performance \n",
       "are often a focus in newer versions.\n",
       "\n",
       "For the most accurate and detailed information, I recommend checking the official announcements or documentation \n",
       "from the developers of the Llama models.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea96bd88-d198-46e6-b6d9-730d7a7c3d92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">MessageRole.ASSISTANT:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'As of my last update in October 2023, specific details about Llama 3.2 compared to Llama 2 have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">not been officially released. However, in general, when comparing iterations of AI models like Llama, you can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expect improvements in several areas:\\n\\n1. **Model Architecture**: Newer versions often incorporate advancements </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in model architecture that enhance performance, efficiency, and scalability.\\n\\n2. **Training Data**: Llama 3.2 may</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be trained on a larger and more diverse dataset, which can improve its understanding of language and context.\\n\\n3.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Performance**: Improvements in natural language understanding, generation quality, and response coherence are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">common in newer models.\\n\\n4. **Fine-tuning and Adaptability**: Enhanced capabilities for fine-tuning on specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks or domains may be introduced, allowing for better customization.\\n\\n5. **Safety and Ethical Considerations**:</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Newer models often include better mechanisms for handling harmful content, bias reduction, and overall ethical AI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">use.\\n\\n6. **Efficiency**: Optimizations that reduce computational requirements while maintaining or improving </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance are often a focus in newer versions.\\n\\nFor the most accurate and detailed information, I recommend </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">checking the official announcements or documentation from the developers of the Llama models.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">raw</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-APXHJVVHV9JHNwiuJ6rNAgtyEQSsT'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'As of my last update in October 2023, specific details about Llama 3.2 compared to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama 2 have not been officially released. However, in general, when comparing iterations of AI models like Llama, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you can expect improvements in several areas:\\n\\n1. **Model Architecture**: Newer versions often incorporate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">advancements in model architecture that enhance performance, efficiency, and scalability.\\n\\n2. **Training Data**: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama 3.2 may be trained on a larger and more diverse dataset, which can improve its understanding of language and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context.\\n\\n3. **Performance**: Improvements in natural language understanding, generation quality, and response </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">coherence are common in newer models.\\n\\n4. **Fine-tuning and Adaptability**: Enhanced capabilities for fine-tuning</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on specific tasks or domains may be introduced, allowing for better customization.\\n\\n5. **Safety and Ethical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Considerations**: Newer models often include better mechanisms for handling harmful content, bias reduction, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">overall ethical AI use.\\n\\n6. **Efficiency**: Optimizations that reduce computational requirements while </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">maintaining or improving performance are often a focus in newer versions.\\n\\nFor the most accurate and detailed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information, I recommend checking the official announcements or documentation from the developers of the Llama </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models.'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">audio</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1730649485</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_0ba0d124f1'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">250</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">271</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reasoning_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">cached_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">delta</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">250</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">271</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmessage\u001b[0m=\u001b[1;35mChatMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mrole\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mMessageRole.ASSISTANT:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'assistant'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'As of my last update in October 2023, specific details about Llama 3.2 compared to Llama 2 have \u001b[0m\n",
       "\u001b[32mnot been officially released. However, in general, when comparing iterations of AI models like Llama, you can \u001b[0m\n",
       "\u001b[32mexpect improvements in several areas:\\n\\n1. **Model Architecture**: Newer versions often incorporate advancements \u001b[0m\n",
       "\u001b[32min model architecture that enhance performance, efficiency, and scalability.\\n\\n2. **Training Data**: Llama 3.2 may\u001b[0m\n",
       "\u001b[32mbe trained on a larger and more diverse dataset, which can improve its understanding of language and context.\\n\\n3.\u001b[0m\n",
       "\u001b[32m**Performance**: Improvements in natural language understanding, generation quality, and response coherence are \u001b[0m\n",
       "\u001b[32mcommon in newer models.\\n\\n4. **Fine-tuning and Adaptability**: Enhanced capabilities for fine-tuning on specific \u001b[0m\n",
       "\u001b[32mtasks or domains may be introduced, allowing for better customization.\\n\\n5. **Safety and Ethical Considerations**:\u001b[0m\n",
       "\u001b[32mNewer models often include better mechanisms for handling harmful content, bias reduction, and overall ethical AI \u001b[0m\n",
       "\u001b[32muse.\\n\\n6. **Efficiency**: Optimizations that reduce computational requirements while maintaining or improving \u001b[0m\n",
       "\u001b[32mperformance are often a focus in newer versions.\\n\\nFor the most accurate and detailed information, I recommend \u001b[0m\n",
       "\u001b[32mchecking the official announcements or documentation from the developers of the Llama models.'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mraw\u001b[0m=\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-APXHJVVHV9JHNwiuJ6rNAgtyEQSsT'\u001b[0m,\n",
       "        \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "                \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mcontent\u001b[0m=\u001b[32m'As of my last update in October 2023, specific details about Llama 3.2 compared to \u001b[0m\n",
       "\u001b[32mLlama 2 have not been officially released. However, in general, when comparing iterations of AI models like Llama, \u001b[0m\n",
       "\u001b[32myou can expect improvements in several areas:\\n\\n1. **Model Architecture**: Newer versions often incorporate \u001b[0m\n",
       "\u001b[32madvancements in model architecture that enhance performance, efficiency, and scalability.\\n\\n2. **Training Data**: \u001b[0m\n",
       "\u001b[32mLlama 3.2 may be trained on a larger and more diverse dataset, which can improve its understanding of language and \u001b[0m\n",
       "\u001b[32mcontext.\\n\\n3. **Performance**: Improvements in natural language understanding, generation quality, and response \u001b[0m\n",
       "\u001b[32mcoherence are common in newer models.\\n\\n4. **Fine-tuning and Adaptability**: Enhanced capabilities for fine-tuning\u001b[0m\n",
       "\u001b[32mon specific tasks or domains may be introduced, allowing for better customization.\\n\\n5. **Safety and Ethical \u001b[0m\n",
       "\u001b[32mConsiderations**: Newer models often include better mechanisms for handling harmful content, bias reduction, and \u001b[0m\n",
       "\u001b[32moverall ethical AI use.\\n\\n6. **Efficiency**: Optimizations that reduce computational requirements while \u001b[0m\n",
       "\u001b[32mmaintaining or improving performance are often a focus in newer versions.\\n\\nFor the most accurate and detailed \u001b[0m\n",
       "\u001b[32minformation, I recommend checking the official announcements or documentation from the developers of the Llama \u001b[0m\n",
       "\u001b[32mmodels.'\u001b[0m,\n",
       "                    \u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                    \u001b[33maudio\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33mcreated\u001b[0m=\u001b[1;36m1730649485\u001b[0m,\n",
       "        \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "        \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "        \u001b[33mservice_tier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_0ba0d124f1'\u001b[0m,\n",
       "        \u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m250\u001b[0m,\n",
       "            \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m21\u001b[0m,\n",
       "            \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m271\u001b[0m,\n",
       "            \u001b[33mcompletion_tokens_details\u001b[0m=\u001b[1;35mCompletionTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33maudio_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mreasoning_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mprompt_tokens_details\u001b[0m=\u001b[1;35mPromptTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33maudio_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mcached_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mdelta\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m, \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m250\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m271\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b528f-ebc1-44e1-a2e5-75c9de362c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b0856d-6a9c-4c58-aa5e-282831744fde",
   "metadata": {},
   "source": [
    "note the model is aliased to `model='gpt-4o-mini-2024-07-18'`.\n",
    "\n",
    "So, the model's training data wont have any information after then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da065824-a2fd-4e0c-8484-9521e651798a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fbbf77-64f2-4931-928a-5477aa8a692b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1040e02-ac2f-46f3-a5c0-a4f3b42cb7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d5940d-0c4b-4d55-8759-22e8ae0f8de4",
   "metadata": {},
   "source": [
    "## Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6948c2-186d-4e9c-a447-dc37d50a76a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Snapshot of Hallucination](../images/llama_2__hallucination.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740a1a3-e484-47a0-8841-c81cdb2b3999",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the above video, we ask about non existant Llama model.\n",
    "\n",
    "We also give it a link to a study music youtube video.\n",
    "\n",
    "Note: we are also using an \"older\" version of Llama model to make the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d96fc7-2c75-462f-be93-0589c351b97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a25d23-7e13-4801-b9cc-91409c0dab70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d003255e-0137-42bd-bfaf-80d8bde17dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = InferenceClient(api_key=os.environ['HF_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a301ed5c-5bcb-469d-b1d3-64ad872cec4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "What did Andrej Karpathy say about Meta's Llama 5.9 in the below youtube talk\n",
    "\n",
    "https://www.youtube.com/watch?v=n61ULEU7CO0&ab_channel=LofiGirl\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b1d5119-bff3-4ff2-93c1-6ae4a6d113ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": message },\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\", \n",
    "    messages=messages, \n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "183225be-de1b-4c1b-8e6d-a37b6e22db93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionOutput</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionOutputComplete</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'length'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionOutputMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"  Andrej Karpathy, a well-known computer scientist and researcher, did not make any </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comments about Meta's Llama 5.9 in the YouTube talk you provided. The talk is actually about the use of language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models in music generation, and Karpathy's comments are focused on the topic of language models and their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">applications in various fields.\\n\\nTherefore, I cannot provide any information about what Karpathy said about </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Meta's Llama\"</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1730649490</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'meta-llama/Llama-2-7b-chat-hf'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2.3.1-dev0-sha-169178b'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionOutputUsage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>, <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">167</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletionOutput\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChatCompletionOutputComplete\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'length'\u001b[0m,\n",
       "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "            \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionOutputMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m\"  Andrej Karpathy, a well-known computer scientist and researcher, did not make any \u001b[0m\n",
       "\u001b[32mcomments about Meta's Llama 5.9 in the YouTube talk you provided. The talk is actually about the use of language \u001b[0m\n",
       "\u001b[32mmodels in music generation, and Karpathy's comments are focused on the topic of language models and their \u001b[0m\n",
       "\u001b[32mapplications in various fields.\\n\\nTherefore, I cannot provide any information about what Karpathy said about \u001b[0m\n",
       "\u001b[32mMeta's Llama\"\u001b[0m,\n",
       "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1730649490\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'meta-llama/Llama-2-7b-chat-hf'\u001b[0m,\n",
       "    \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'2.3.1-dev0-sha-169178b'\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mChatCompletionOutputUsage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m100\u001b[0m, \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m67\u001b[0m, \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m167\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4e9a249-66ed-4936-890e-6850a631684a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Andrej Karpathy, a well-known computer scientist and researcher, did not make any comments about Meta's Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.9</span>\n",
       "in the YouTube talk you provided. The talk is actually about the use of language models in music generation, and \n",
       "Karpathy's comments are focused on the topic of language models and their applications in various fields.\n",
       "\n",
       "Therefore, I cannot provide any information about what Karpathy said about Meta's Llama\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Andrej Karpathy, a well-known computer scientist and researcher, did not make any comments about Meta's Llama \u001b[1;36m5.9\u001b[0m\n",
       "in the YouTube talk you provided. The talk is actually about the use of language models in music generation, and \n",
       "Karpathy's comments are focused on the topic of language models and their applications in various fields.\n",
       "\n",
       "Therefore, I cannot provide any information about what Karpathy said about Meta's Llama\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8197d2-3326-4857-87a3-5e22d07e82b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6b42b54-f7f4-4689-ad21-108e552ec308",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798509d-a073-42fe-9315-2b873a9f1d67",
   "metadata": {},
   "source": [
    "We explored at least two problems with directly using LLMs directly. \n",
    "\n",
    "- Knowledge Cuttoff  \n",
    "- Hallucination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb31ae-4e15-4c8a-90c8-d754ec50eccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
