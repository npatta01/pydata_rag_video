{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4eab10f-af05-453a-b335-a5259f3f4b1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook : Why Rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dae9c54-fc1a-476c-ba06-fd0b3ef3cff4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e4a1d-97ea-4913-a562-8a0a88b2715b",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this notebook, we will motivate Retrieval Augmented Generation (RAG) .\n",
    "\n",
    "We will see why we can't use Large Language Model directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1fbb5-5111-4dfe-8ab6-c55db01673be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2fd037-bdbc-4b98-815c-e650930a43d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0bd6025-261b-4735-8c0e-68f7f23c3fd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb2c26-9588-4c17-92f7-e86a34287bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f16f4c-bf75-40d2-9c85-c6e99b1fc56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import rich\n",
    "import logging\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from huggingface_hub import InferenceClient\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b3d6d8-0786-4e50-b879-ed76b81dd650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#logging.basicConfig(level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0dff8e-e2d4-4874-9529-05404682dd75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv_path = \"../env\"\n",
    "if os.path.isfile(\"env\"):\n",
    "    dotenv_path = \"env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32606a-c066-4f71-bf4b-18c1ba9db121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08eae1-d3e9-4ee2-8d29-2b50d595fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e03fdea-fd48-444e-96a9-51fd0e8db5da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0713199-2b56-4cbf-95fc-5f15c0722227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620eb07-8ec4-4f64-84db-f4cc14082f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60b96e58-e4fc-4f03-9107-776d189ffe67",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04fc47-e448-4ad6-9e22-46cac490f5cc",
   "metadata": {},
   "source": [
    "There are atleast two problems with using Large Language Model \n",
    "\n",
    "- Knoweldge Cutoff\n",
    "- Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b635b-1dcb-45fb-b321-10fd46c086c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8f41792-ad94-487b-8e17-0a0f465581b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Knoweldge Cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be027950-9c80-4a6e-a293-59b0359c2451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850da7c3-80fe-4b7d-87c7-ff5950cbe48f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\",temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e61e4e8-0646-45fb-8593-2d1a3886b808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response(query:str):\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=query),\n",
    "    ]\n",
    "    \n",
    "    resp = llm.chat(messages)\n",
    "\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fff0b3-1bcb-45e8-8ad8-adfbc2edd5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cab561-8afe-4069-868f-bfc3b94fe7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5e41e1-cc61-400f-b443-5d220e36e97d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query=\"what is different about Llama3.2 than Llama2 ?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc54077-17a9-48c7-a87c-e6ed0df6e46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = get_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a84f78-1244-47d3-b92f-a0c3931a564e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">As of my last update in October <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>, specific details about Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span> compared to Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> were not available. \n",
       "However, in general, when comparing iterations of AI models like Llama, you can expect improvements in several \n",
       "areas:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Model Architecture**: Newer versions often incorporate advancements in model architecture that enhance \n",
       "performance, efficiency, and scalability.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Training Data**: Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span> may have been trained on a larger and more diverse dataset, which can improve its \n",
       "understanding of language and context.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Fine-tuning and Specialization**: The newer model might include better fine-tuning techniques or specialized \n",
       "training for specific tasks, leading to improved performance in those areas.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Performance Metrics**: Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span> could show improvements in various benchmarks, such as accuracy, coherence, \n",
       "and response time.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. **Safety and Ethical Considerations**: Newer models often incorporate better safety measures to reduce biases \n",
       "and improve the ethical use of AI.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. **User Feedback Integration**: Updates may include adjustments based on user feedback from previous versions, \n",
       "leading to a more user-friendly experience.\n",
       "\n",
       "For the most accurate and detailed information, I recommend checking the official release notes or documentation \n",
       "from the developers of the Llama models.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "As of my last update in October \u001b[1;36m2023\u001b[0m, specific details about Llama \u001b[1;36m3.2\u001b[0m compared to Llama \u001b[1;36m2\u001b[0m were not available. \n",
       "However, in general, when comparing iterations of AI models like Llama, you can expect improvements in several \n",
       "areas:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Model Architecture**: Newer versions often incorporate advancements in model architecture that enhance \n",
       "performance, efficiency, and scalability.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Training Data**: Llama \u001b[1;36m3.2\u001b[0m may have been trained on a larger and more diverse dataset, which can improve its \n",
       "understanding of language and context.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Fine-tuning and Specialization**: The newer model might include better fine-tuning techniques or specialized \n",
       "training for specific tasks, leading to improved performance in those areas.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. **Performance Metrics**: Llama \u001b[1;36m3.2\u001b[0m could show improvements in various benchmarks, such as accuracy, coherence, \n",
       "and response time.\n",
       "\n",
       "\u001b[1;36m5\u001b[0m. **Safety and Ethical Considerations**: Newer models often incorporate better safety measures to reduce biases \n",
       "and improve the ethical use of AI.\n",
       "\n",
       "\u001b[1;36m6\u001b[0m. **User Feedback Integration**: Updates may include adjustments based on user feedback from previous versions, \n",
       "leading to a more user-friendly experience.\n",
       "\n",
       "For the most accurate and detailed information, I recommend checking the official release notes or documentation \n",
       "from the developers of the Llama models.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea96bd88-d198-46e6-b6d9-730d7a7c3d92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">MessageRole.ASSISTANT:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'As of my last update in October 2023, specific details about Llama 3.2 compared to Llama 2 were </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">not available. However, in general, when comparing iterations of AI models like Llama, you can expect improvements </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in several areas:\\n\\n1. **Model Architecture**: Newer versions often incorporate advancements in model architecture</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that enhance performance, efficiency, and scalability.\\n\\n2. **Training Data**: Llama 3.2 may have been trained on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a larger and more diverse dataset, which can improve its understanding of language and context.\\n\\n3. **Fine-tuning</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and Specialization**: The newer model might include better fine-tuning techniques or specialized training for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific tasks, leading to improved performance in those areas.\\n\\n4. **Performance Metrics**: Llama 3.2 could show</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">improvements in various benchmarks, such as accuracy, coherence, and response time.\\n\\n5. **Safety and Ethical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Considerations**: Newer models often incorporate better safety measures to reduce biases and improve the ethical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">use of AI.\\n\\n6. **User Feedback Integration**: Updates may include adjustments based on user feedback from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">previous versions, leading to a more user-friendly experience.\\n\\nFor the most accurate and detailed information, I</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recommend checking the official release notes or documentation from the developers of the Llama models.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">raw</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-AQbXyWpmhKNII9KOW1JFHGh5N0Hge'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'As of my last update in October 2023, specific details about Llama 3.2 compared to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama 2 were not available. However, in general, when comparing iterations of AI models like Llama, you can expect </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">improvements in several areas:\\n\\n1. **Model Architecture**: Newer versions often incorporate advancements in model</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">architecture that enhance performance, efficiency, and scalability.\\n\\n2. **Training Data**: Llama 3.2 may have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">been trained on a larger and more diverse dataset, which can improve its understanding of language and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context.\\n\\n3. **Fine-tuning and Specialization**: The newer model might include better fine-tuning techniques or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specialized training for specific tasks, leading to improved performance in those areas.\\n\\n4. **Performance </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Metrics**: Llama 3.2 could show improvements in various benchmarks, such as accuracy, coherence, and response </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">time.\\n\\n5. **Safety and Ethical Considerations**: Newer models often incorporate better safety measures to reduce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">biases and improve the ethical use of AI.\\n\\n6. **User Feedback Integration**: Updates may include adjustments </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">based on user feedback from previous versions, leading to a more user-friendly experience.\\n\\nFor the most accurate</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and detailed information, I recommend checking the official release notes or documentation from the developers of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the Llama models.'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">audio</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1730904222</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_0ba0d124f1'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">283</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionTokensDetails</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">reasoning_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">accepted_prediction_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">rejected_prediction_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">cached_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">delta</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">283</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmessage\u001b[0m=\u001b[1;35mChatMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mrole\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mMessageRole.ASSISTANT:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'assistant'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'As of my last update in October 2023, specific details about Llama 3.2 compared to Llama 2 were \u001b[0m\n",
       "\u001b[32mnot available. However, in general, when comparing iterations of AI models like Llama, you can expect improvements \u001b[0m\n",
       "\u001b[32min several areas:\\n\\n1. **Model Architecture**: Newer versions often incorporate advancements in model architecture\u001b[0m\n",
       "\u001b[32mthat enhance performance, efficiency, and scalability.\\n\\n2. **Training Data**: Llama 3.2 may have been trained on \u001b[0m\n",
       "\u001b[32ma larger and more diverse dataset, which can improve its understanding of language and context.\\n\\n3. **Fine-tuning\u001b[0m\n",
       "\u001b[32mand Specialization**: The newer model might include better fine-tuning techniques or specialized training for \u001b[0m\n",
       "\u001b[32mspecific tasks, leading to improved performance in those areas.\\n\\n4. **Performance Metrics**: Llama 3.2 could show\u001b[0m\n",
       "\u001b[32mimprovements in various benchmarks, such as accuracy, coherence, and response time.\\n\\n5. **Safety and Ethical \u001b[0m\n",
       "\u001b[32mConsiderations**: Newer models often incorporate better safety measures to reduce biases and improve the ethical \u001b[0m\n",
       "\u001b[32muse of AI.\\n\\n6. **User Feedback Integration**: Updates may include adjustments based on user feedback from \u001b[0m\n",
       "\u001b[32mprevious versions, leading to a more user-friendly experience.\\n\\nFor the most accurate and detailed information, I\u001b[0m\n",
       "\u001b[32mrecommend checking the official release notes or documentation from the developers of the Llama models.'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mraw\u001b[0m=\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AQbXyWpmhKNII9KOW1JFHGh5N0Hge'\u001b[0m,\n",
       "        \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "                \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mcontent\u001b[0m=\u001b[32m'As of my last update in October 2023, specific details about Llama 3.2 compared to \u001b[0m\n",
       "\u001b[32mLlama 2 were not available. However, in general, when comparing iterations of AI models like Llama, you can expect \u001b[0m\n",
       "\u001b[32mimprovements in several areas:\\n\\n1. **Model Architecture**: Newer versions often incorporate advancements in model\u001b[0m\n",
       "\u001b[32marchitecture that enhance performance, efficiency, and scalability.\\n\\n2. **Training Data**: Llama 3.2 may have \u001b[0m\n",
       "\u001b[32mbeen trained on a larger and more diverse dataset, which can improve its understanding of language and \u001b[0m\n",
       "\u001b[32mcontext.\\n\\n3. **Fine-tuning and Specialization**: The newer model might include better fine-tuning techniques or \u001b[0m\n",
       "\u001b[32mspecialized training for specific tasks, leading to improved performance in those areas.\\n\\n4. **Performance \u001b[0m\n",
       "\u001b[32mMetrics**: Llama 3.2 could show improvements in various benchmarks, such as accuracy, coherence, and response \u001b[0m\n",
       "\u001b[32mtime.\\n\\n5. **Safety and Ethical Considerations**: Newer models often incorporate better safety measures to reduce \u001b[0m\n",
       "\u001b[32mbiases and improve the ethical use of AI.\\n\\n6. **User Feedback Integration**: Updates may include adjustments \u001b[0m\n",
       "\u001b[32mbased on user feedback from previous versions, leading to a more user-friendly experience.\\n\\nFor the most accurate\u001b[0m\n",
       "\u001b[32mand detailed information, I recommend checking the official release notes or documentation from the developers of \u001b[0m\n",
       "\u001b[32mthe Llama models.'\u001b[0m,\n",
       "                    \u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                    \u001b[33maudio\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33mcreated\u001b[0m=\u001b[1;36m1730904222\u001b[0m,\n",
       "        \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "        \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "        \u001b[33mservice_tier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_0ba0d124f1'\u001b[0m,\n",
       "        \u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m262\u001b[0m,\n",
       "            \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m21\u001b[0m,\n",
       "            \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m283\u001b[0m,\n",
       "            \u001b[33mcompletion_tokens_details\u001b[0m=\u001b[1;35mCompletionTokensDetails\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33maudio_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "                \u001b[33mreasoning_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "                \u001b[33maccepted_prediction_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "                \u001b[33mrejected_prediction_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33mprompt_tokens_details\u001b[0m=\u001b[1;35mPromptTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33maudio_tokens\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mcached_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mdelta\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m, \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m262\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m283\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b528f-ebc1-44e1-a2e5-75c9de362c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b0856d-6a9c-4c58-aa5e-282831744fde",
   "metadata": {},
   "source": [
    "note the model is aliased to `model='gpt-4o-mini-2024-07-18'`.\n",
    "\n",
    "So, the model's training data wont have any information after then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da065824-a2fd-4e0c-8484-9521e651798a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fbbf77-64f2-4931-928a-5477aa8a692b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1040e02-ac2f-46f3-a5c0-a4f3b42cb7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d5940d-0c4b-4d55-8759-22e8ae0f8de4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6948c2-186d-4e9c-a447-dc37d50a76a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Snapshot of Hallucination](../images/llama_2__hallucination.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740a1a3-e484-47a0-8841-c81cdb2b3999",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the above video, we ask about non existant Llama model.\n",
    "\n",
    "We also give it a link to a study music youtube video.\n",
    "\n",
    "Note: we are also using an \"older\" version of Llama model to make the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d96fc7-2c75-462f-be93-0589c351b97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a25d23-7e13-4801-b9cc-91409c0dab70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d003255e-0137-42bd-bfaf-80d8bde17dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = InferenceClient(api_key=os.environ['HF_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a301ed5c-5bcb-469d-b1d3-64ad872cec4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "What did Andrej Karpathy say about Meta's Llama 5.9 in the below youtube talk\n",
    "\n",
    "https://www.youtube.com/watch?v=n61ULEU7CO0&ab_channel=LofiGirl\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b1d5119-bff3-4ff2-93c1-6ae4a6d113ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": message },\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\", \n",
    "    messages=messages, \n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "183225be-de1b-4c1b-8e6d-a37b6e22db93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionOutput</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionOutputComplete</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'length'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionOutputMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"  Andrej Karpathy, a well-known computer scientist and researcher, did not make any </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comments about Meta's Llama 5.9 in the YouTube talk you provided. The talk is actually about the use of language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models in music generation, and Karpathy's comments are focused on the topic of language models and their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">applications in various fields.\\n\\nTherefore, I cannot provide any information about what Karpathy said about </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Meta's Llama\"</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1730904228</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'meta-llama/Llama-2-7b-chat-hf'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2.3.1-dev0-sha-169178b'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionOutputUsage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>, <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">167</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletionOutput\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChatCompletionOutputComplete\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'length'\u001b[0m,\n",
       "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "            \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionOutputMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m\"  Andrej Karpathy, a well-known computer scientist and researcher, did not make any \u001b[0m\n",
       "\u001b[32mcomments about Meta's Llama 5.9 in the YouTube talk you provided. The talk is actually about the use of language \u001b[0m\n",
       "\u001b[32mmodels in music generation, and Karpathy's comments are focused on the topic of language models and their \u001b[0m\n",
       "\u001b[32mapplications in various fields.\\n\\nTherefore, I cannot provide any information about what Karpathy said about \u001b[0m\n",
       "\u001b[32mMeta's Llama\"\u001b[0m,\n",
       "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1730904228\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'meta-llama/Llama-2-7b-chat-hf'\u001b[0m,\n",
       "    \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'2.3.1-dev0-sha-169178b'\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mChatCompletionOutputUsage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m100\u001b[0m, \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m67\u001b[0m, \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m167\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4e9a249-66ed-4936-890e-6850a631684a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Andrej Karpathy, a well-known computer scientist and researcher, did not make any comments about Meta's Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.9</span>\n",
       "in the YouTube talk you provided. The talk is actually about the use of language models in music generation, and \n",
       "Karpathy's comments are focused on the topic of language models and their applications in various fields.\n",
       "\n",
       "Therefore, I cannot provide any information about what Karpathy said about Meta's Llama\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Andrej Karpathy, a well-known computer scientist and researcher, did not make any comments about Meta's Llama \u001b[1;36m5.9\u001b[0m\n",
       "in the YouTube talk you provided. The talk is actually about the use of language models in music generation, and \n",
       "Karpathy's comments are focused on the topic of language models and their applications in various fields.\n",
       "\n",
       "Therefore, I cannot provide any information about what Karpathy said about Meta's Llama\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8197d2-3326-4857-87a3-5e22d07e82b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6b42b54-f7f4-4689-ad21-108e552ec308",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798509d-a073-42fe-9315-2b873a9f1d67",
   "metadata": {},
   "source": [
    "We explored at least two problems with directly using LLMs directly. \n",
    "\n",
    "- Knowledge Cuttoff  \n",
    "- Hallucination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb31ae-4e15-4c8a-90c8-d754ec50eccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
