{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699738e-1c20-4ff9-b1ef-6feaa9f23a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475dba4-c84c-4894-9490-ed3fb86fd324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38b55a-f3d0-4700-b96f-e534e49947ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a826d86-0c2a-4a15-a078-e0996376da4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2badcac-c8f6-4388-ad3d-93f96aa65b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytubefix import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a67ba063-42af-431f-ba20-b1673557838f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yt = YouTube('http://youtube.com/watch?v=TQQlZhbC5ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c2c2708-2fa6-4303-8e7a-fcb828e70582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CodeEmporium'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e513100c-4cc3-4e40-a331-1823719f61be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e74e89d-7c26-4724-afd6-cf382b23c028",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please subscribe to keep me alive: https://www.youtube.com/c/CodeEmporium?sub_confirmation=1\\n\\nBLOG: https://medium.com/@dataemporium\\n\\n\\nPLAYLISTS FROM MY CHANNEL\\n‚≠ï Reinforcement Learning: https://youtube.com/playlist?list=PLTl9hO2Oobd9kS--NgVz0EPNyEmygV1Ha&si=AuThDZJwG19cgTA8\\nNatural Language Processing: https://youtube.com/playlist?list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE&si=LsVy8RDPu8jeO-cc\\n‚≠ï Transformers from Scratch: https://youtube.com/playlist?list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE\\n‚≠ï ChatGPT Playlist: https://youtube.com/playlist?list=PLTl9hO2Oobd9coYT6XsTraTBo4pL1j4HJ\\n‚≠ï  Convolutional Neural Networks: https://youtube.com/playlist?list=PLTl9hO2Oobd9U0XHz62Lw6EgIMkQpfz74\\n‚≠ï  The Math You Should Know : https://youtube.com/playlist?list=PLTl9hO2Oobd-_5sGLnbgE8Poer1Xjzz4h\\n‚≠ï Probability Theory for Machine Learning: https://youtube.com/playlist?list=PLTl9hO2Oobd9bPcq0fj91Jgk_-h1H_W3V\\n‚≠ï Coding Machine Learning: https://youtube.com/playlist?list=PLTl9hO2Oobd82vcsOnvCNzxrZOlrz3RiD\\n\\n\\nMATH COURSES (7 day free trial)\\nüìï Mathematics for Machine Learning: https://imp.i384100.net/MathML\\nüìï Calculus: https://imp.i384100.net/Calculus\\nüìï Statistics for Data Science: https://imp.i384100.net/AdvancedStatistics\\nüìï Bayesian Statistics: https://imp.i384100.net/BayesianStatistics\\nüìï Linear Algebra: https://imp.i384100.net/LinearAlgebra\\nüìï Probability: https://imp.i384100.net/Probability\\n\\nOTHER RELATED COURSES (7 day free trial)\\nüìï ‚≠ê Deep Learning Specialization: https://imp.i384100.net/Deep-Learning\\nüìï Python for Everybody: https://imp.i384100.net/python\\nüìï MLOps Course: https://imp.i384100.net/MLOps\\nüìï Natural Language Processing (NLP): https://imp.i384100.net/NLP\\nüìï Machine Learning in Production: https://imp.i384100.net/MLProduction\\nüìï Data Science Specialization: https://imp.i384100.net/DataScience\\nüìï Tensorflow: https://imp.i384100.net/Tensorflow\\n\\nREFERENCES\\n[1] The main Paper: https://arxiv.org/abs/1706.03762\\n[2] Tensor2Tensor has some code with a tutorial: https://www.tensorflow.org/tutorials/text/transformer\\n[3] Transformer very intuitively explained - Amazing: http://jalammar.github.io/illustrated-transformer/\\n[4] Medium Blog on intuitive explanation: https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04\\n[5] Pretrained word embeddings: https://nlp.stanford.edu/projects/glove/\\n[6] Intuitive explanation of Layer normalization: https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/\\n[7] Paper that gives even better results than transformers (Pervasive Attention): https://arxiv.org/abs/1808.03867\\n[8] BERT uses transformers to pretrain neural nets for common NLP tasks. : https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\\n[9] Stanford Lecture on RNN: http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture10.pdf\\n[10] Colah‚Äôs Blog: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\\n[11] Wiki for timeseries of events: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e50975f-cb18-4529-b7e0-e453a964e29a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KeyMoment: Recurrent Neural Networks | 0:00:00>,\n",
       " <KeyMoment: Transformers | 0:02:46>,\n",
       " <KeyMoment: English-French Translation | 0:03:01>,\n",
       " <KeyMoment: Transformer Components | 0:04:09>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.key_moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d6a3b86-0a62-4ede-8184-558732f07150",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine Learning',\n",
       " 'Deep Learning',\n",
       " 'Data Science',\n",
       " 'Artificial Intelligence',\n",
       " 'Neural Network',\n",
       " 'attention',\n",
       " 'attention is all you need',\n",
       " 'attention neural networks',\n",
       " 'transformer neural networks',\n",
       " 'most important paper in deep learning']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "149400bf-f41b-4e78-9e19-e2c669584e55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TQQlZhbC5ps'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3216d707-6907-47cd-96eb-f119f46be1ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 1, 13, 7, 0, 6, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=57600)))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.publish_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2fbbdab5-19be-4853-8c3e-73d83954b046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformer Neural Networks - EXPLAINED! (Attention is all you need)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "305e39b2-1b82-4803-99e5-eff1ccc38ece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "523a31a0-d035-4dd2-9bf3-c73c8b0660fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yt.likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f62e52c3-9a38-415a-80e1-cf8000740560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yt.rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "268c289e-edda-410d-868c-24a643b36a19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810097"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "017898ab-6727-49aa-8f36-bf4b8a1fe5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UC5_6ZD6s8klmMu9TXEB_1IA'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.channel_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c269ae5a-2a98-4fc1-af4c-f056bcbba029",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://i.ytimg.com/vi/TQQlZhbC5ps/sddefault.jpg'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.thumbnail_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "818876a3-6764-4261-8e34-2bc6a12a129f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://youtube.com/watch?v=TQQlZhbC5ps'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.watch_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1a6f1f9-0f5b-44bb-8abe-92d8f87d94db",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start_seconds': 0.0, 'duration': 7.85, 'norm_intensity': 0.30715709198047353}, {'start_seconds': 7.85, 'duration': 7.85, 'norm_intensity': 0.3279508738568646}, {'start_seconds': 15.7, 'duration': 7.85, 'norm_intensity': 0.3148020213047231}, {'start_seconds': 23.55, 'duration': 7.85, 'norm_intensity': 0.3828431918082935}, {'start_seconds': 31.4, 'duration': 7.85, 'norm_intensity': 0.40334767917262904}, {'start_seconds': 39.25, 'duration': 7.85, 'norm_intensity': 0.29380590330025286}, {'start_seconds': 47.1, 'duration': 7.85, 'norm_intensity': 0.2516466529462962}, {'start_seconds': 54.95, 'duration': 7.85, 'norm_intensity': 0.1912189763921347}, {'start_seconds': 62.8, 'duration': 7.85, 'norm_intensity': 0.19321351347861024}, {'start_seconds': 70.65, 'duration': 7.85, 'norm_intensity': 0.2070582962447864}, {'start_seconds': 78.5, 'duration': 7.85, 'norm_intensity': 0.18223971357819135}, {'start_seconds': 86.35, 'duration': 7.85, 'norm_intensity': 0.23299968653688682}, {'start_seconds': 94.2, 'duration': 7.85, 'norm_intensity': 0.3028129782230378}, {'start_seconds': 102.05, 'duration': 7.85, 'norm_intensity': 0.30264703668862475}, {'start_seconds': 109.9, 'duration': 7.85, 'norm_intensity': 0.2917368362394288}, {'start_seconds': 117.75, 'duration': 7.85, 'norm_intensity': 0.3023881452242257}, {'start_seconds': 125.6, 'duration': 7.85, 'norm_intensity': 0.22832093683813406}, {'start_seconds': 133.45, 'duration': 7.85, 'norm_intensity': 0.18562297362776492}, {'start_seconds': 141.3, 'duration': 7.85, 'norm_intensity': 0.2144394963360684}, {'start_seconds': 149.15, 'duration': 7.85, 'norm_intensity': 0.2531069303524474}, {'start_seconds': 157.0, 'duration': 7.85, 'norm_intensity': 0.3543071989518357}, {'start_seconds': 164.85, 'duration': 7.85, 'norm_intensity': 0.4676127182871012}, {'start_seconds': 172.7, 'duration': 7.85, 'norm_intensity': 0.385546269693817}, {'start_seconds': 180.55, 'duration': 7.85, 'norm_intensity': 0.32349992435673164}, {'start_seconds': 188.4, 'duration': 7.85, 'norm_intensity': 0.31733988635737526}, {'start_seconds': 196.25, 'duration': 7.85, 'norm_intensity': 0.30964143795322957}, {'start_seconds': 204.1, 'duration': 7.85, 'norm_intensity': 0.31311115034404224}, {'start_seconds': 211.95, 'duration': 7.85, 'norm_intensity': 0.337394238586034}, {'start_seconds': 219.8, 'duration': 7.85, 'norm_intensity': 0.3702369385207869}, {'start_seconds': 227.65, 'duration': 7.85, 'norm_intensity': 0.4668187574739982}, {'start_seconds': 235.5, 'duration': 7.85, 'norm_intensity': 0.5072486978609493}, {'start_seconds': 243.35, 'duration': 7.85, 'norm_intensity': 0.6101157295450137}, {'start_seconds': 251.2, 'duration': 7.85, 'norm_intensity': 0.7677470301262626}, {'start_seconds': 259.05, 'duration': 7.85, 'norm_intensity': 0.8203940162106135}, {'start_seconds': 266.9, 'duration': 7.85, 'norm_intensity': 0.8017724531466163}, {'start_seconds': 274.75, 'duration': 7.85, 'norm_intensity': 0.8161686808044751}, {'start_seconds': 282.6, 'duration': 7.85, 'norm_intensity': 0.8130879735866743}, {'start_seconds': 290.45, 'duration': 7.85, 'norm_intensity': 0.6119662671101652}, {'start_seconds': 298.3, 'duration': 7.85, 'norm_intensity': 0.5329763154591077}, {'start_seconds': 306.15, 'duration': 7.85, 'norm_intensity': 0.6346288850570883}, {'start_seconds': 314.0, 'duration': 7.85, 'norm_intensity': 0.7073724070926788}, {'start_seconds': 321.85, 'duration': 7.85, 'norm_intensity': 0.7492257849745062}, {'start_seconds': 329.7, 'duration': 7.85, 'norm_intensity': 0.902972741987714}, {'start_seconds': 337.55, 'duration': 7.85, 'norm_intensity': 0.8542639829020708}, {'start_seconds': 345.4, 'duration': 7.85, 'norm_intensity': 0.756689227131453}, {'start_seconds': 353.25, 'duration': 7.85, 'norm_intensity': 0.7246911327669514}, {'start_seconds': 361.1, 'duration': 7.85, 'norm_intensity': 0.7027514677162344}, {'start_seconds': 368.95, 'duration': 7.85, 'norm_intensity': 0.6603576361909783}, {'start_seconds': 376.8, 'duration': 7.85, 'norm_intensity': 0.6371302745492363}, {'start_seconds': 384.65, 'duration': 7.85, 'norm_intensity': 0.6820973941187551}, {'start_seconds': 392.5, 'duration': 7.85, 'norm_intensity': 0.7111443282062028}, {'start_seconds': 400.35, 'duration': 7.85, 'norm_intensity': 0.710611323511867}, {'start_seconds': 408.2, 'duration': 7.85, 'norm_intensity': 0.66289392239029}, {'start_seconds': 416.05, 'duration': 7.85, 'norm_intensity': 0.5802220847495276}, {'start_seconds': 423.9, 'duration': 7.85, 'norm_intensity': 0.5766489777066314}, {'start_seconds': 431.75, 'duration': 7.85, 'norm_intensity': 0.6026221272409242}, {'start_seconds': 439.6, 'duration': 7.85, 'norm_intensity': 0.5475957282058223}, {'start_seconds': 447.45, 'duration': 7.85, 'norm_intensity': 0.4718923824415153}, {'start_seconds': 455.3, 'duration': 7.85, 'norm_intensity': 0.45377329957394036}, {'start_seconds': 463.15, 'duration': 7.85, 'norm_intensity': 0.445143449149243}, {'start_seconds': 471.0, 'duration': 7.85, 'norm_intensity': 0.394819604063256}, {'start_seconds': 478.85, 'duration': 7.85, 'norm_intensity': 0.3498696899468052}, {'start_seconds': 486.7, 'duration': 7.85, 'norm_intensity': 0.28930706191546246}, {'start_seconds': 494.55, 'duration': 7.85, 'norm_intensity': 0.34946424850568614}, {'start_seconds': 502.4, 'duration': 7.85, 'norm_intensity': 0.45615182188947373}, {'start_seconds': 510.25, 'duration': 7.85, 'norm_intensity': 0.568543994809054}, {'start_seconds': 518.1, 'duration': 7.85, 'norm_intensity': 0.8601524581268865}, {'start_seconds': 525.95, 'duration': 7.85, 'norm_intensity': 1.0}, {'start_seconds': 533.8, 'duration': 7.85, 'norm_intensity': 0.8034374552025667}, {'start_seconds': 541.65, 'duration': 7.85, 'norm_intensity': 0.5727039494935342}, {'start_seconds': 549.5, 'duration': 7.85, 'norm_intensity': 0.5243240372202935}, {'start_seconds': 557.35, 'duration': 7.85, 'norm_intensity': 0.5833757026049178}, {'start_seconds': 565.2, 'duration': 7.85, 'norm_intensity': 0.6069207376354226}, {'start_seconds': 573.05, 'duration': 7.85, 'norm_intensity': 0.562251454579365}, {'start_seconds': 580.9, 'duration': 7.85, 'norm_intensity': 0.6219122334860139}, {'start_seconds': 588.75, 'duration': 7.85, 'norm_intensity': 0.5982410521219567}, {'start_seconds': 596.6, 'duration': 7.85, 'norm_intensity': 0.5081073511775878}, {'start_seconds': 604.45, 'duration': 7.85, 'norm_intensity': 0.4843170271114623}, {'start_seconds': 612.3, 'duration': 7.85, 'norm_intensity': 0.4575452611708833}, {'start_seconds': 620.15, 'duration': 7.85, 'norm_intensity': 0.3671956621530758}, {'start_seconds': 628.0, 'duration': 7.85, 'norm_intensity': 0.2998559073668123}, {'start_seconds': 635.85, 'duration': 7.85, 'norm_intensity': 0.44080419340208526}, {'start_seconds': 643.7, 'duration': 7.85, 'norm_intensity': 0.5746359801810993}, {'start_seconds': 651.55, 'duration': 7.85, 'norm_intensity': 0.5858856340984713}, {'start_seconds': 659.4, 'duration': 7.85, 'norm_intensity': 0.6292640223734044}, {'start_seconds': 667.25, 'duration': 7.85, 'norm_intensity': 0.59928827720422}, {'start_seconds': 675.1, 'duration': 7.85, 'norm_intensity': 0.539781456739965}, {'start_seconds': 682.95, 'duration': 7.85, 'norm_intensity': 0.5143572623670873}, {'start_seconds': 690.8, 'duration': 7.85, 'norm_intensity': 0.44361969374212573}, {'start_seconds': 698.65, 'duration': 7.85, 'norm_intensity': 0.38898234036345447}, {'start_seconds': 706.5, 'duration': 7.85, 'norm_intensity': 0.3470830542844999}, {'start_seconds': 714.35, 'duration': 7.85, 'norm_intensity': 0.32363898490093973}, {'start_seconds': 722.2, 'duration': 7.85, 'norm_intensity': 0.2605323788206783}, {'start_seconds': 730.05, 'duration': 7.85, 'norm_intensity': 0.31174220353111765}, {'start_seconds': 737.9, 'duration': 7.85, 'norm_intensity': 0.3193868089880153}, {'start_seconds': 745.75, 'duration': 7.85, 'norm_intensity': 0.30095094336689504}, {'start_seconds': 753.6, 'duration': 7.85, 'norm_intensity': 0.18019799306689063}, {'start_seconds': 761.45, 'duration': 7.85, 'norm_intensity': 0.06312638223043478}, {'start_seconds': 769.3, 'duration': 7.85, 'norm_intensity': 0.014344677084210895}, {'start_seconds': 777.15, 'duration': 7.85, 'norm_intensity': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "print(yt.replayed_heatmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1833bf95-ded7-4fd8-a42c-5f7bc6144522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbae96e-1774-4a0b-9ad8-a26f0b4436d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8dcfa-33e5-4e27-b3f3-453427f0fe22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb2ac2-4467-4504-ae7b-5babe55f4c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bedb20-52be-46a7-ac95-fb810e014fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c38b07-e0e5-4b10-8e4a-8a3f3bce8c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70519711-b293-4ce4-8cce-32b9ec907f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa272986-ac85-4a20-ba6e-fc82e5b71402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "96aa8e83-88a8-4718-b29d-e46ea34de33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "03c8edaa-3998-4f41-b1a0-7cadf8c9d295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_url = \"https://youtube.com/watch?v=TQQlZhbC5ps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "73c43ff3-dd38-4fc3-9423-388a080184c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_details = get_video_metadata (video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2faf0bb7-31c4-40d9-a94a-be315488eda6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'video_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'TQQlZhbC5ps'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Transformer Neural Networks - EXPLAINED! (Attention is all you need)'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'CodeEmporium'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Machine Learning'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Deep Learning'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Data Science'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Artificial Intelligence'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Neural Network'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'attention'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'attention is all you need'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'attention neural networks'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'transformer neural networks'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'most important paper in deep learning'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'publish_date'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #808000; text-decoration-color: #808000\">tzinfo</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.timezone</span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.timedelta</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">days</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">seconds</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57600</span><span style=\"font-weight: bold\">)))</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">785</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'likes'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'views'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">810097</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'channel_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'UC5_6ZD6s8klmMu9TXEB_1IA'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'thumbnail_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://i.ytimg.com/vi/TQQlZhbC5ps/sddefault.jpg'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Please subscribe to keep me alive: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://www.youtube.com/c/CodeEmporium?sub_confirmation=1\\n\\nBLOG: https://medium.com/@dataemporium\\n\\n\\nPLAYLISTS </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">FROM MY CHANNEL\\n‚≠ï Reinforcement Learning: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://youtube.com/playlist?list=PLTl9hO2Oobd9kS--NgVz0EPNyEmygV1Ha&amp;si=AuThDZJwG19cgTA8\\nNatural Language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Processing: https://youtube.com/playlist?list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE&amp;si=LsVy8RDPu8jeO-cc\\n‚≠ï </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Transformers from Scratch: https://youtube.com/playlist?list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE\\n‚≠ï ChatGPT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Playlist: https://youtube.com/playlist?list=PLTl9hO2Oobd9coYT6XsTraTBo4pL1j4HJ\\n‚≠ï  Convolutional Neural Networks: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://youtube.com/playlist?list=PLTl9hO2Oobd9U0XHz62Lw6EgIMkQpfz74\\n‚≠ï  The Math You Should Know : </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://youtube.com/playlist?list=PLTl9hO2Oobd-_5sGLnbgE8Poer1Xjzz4h\\n‚≠ï Probability Theory for Machine Learning: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://youtube.com/playlist?list=PLTl9hO2Oobd9bPcq0fj91Jgk_-h1H_W3V\\n‚≠ï Coding Machine Learning: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://youtube.com/playlist?list=PLTl9hO2Oobd82vcsOnvCNzxrZOlrz3RiD\\n\\n\\nMATH COURSES (7 day free trial)\\nüìï </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Mathematics for Machine Learning: https://imp.i384100.net/MathML\\nüìï Calculus: https://imp.i384100.net/Calculus\\nüìï</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Statistics for Data Science: https://imp.i384100.net/AdvancedStatistics\\nüìï Bayesian Statistics: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://imp.i384100.net/BayesianStatistics\\nüìï Linear Algebra: https://imp.i384100.net/LinearAlgebra\\nüìï </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Probability: https://imp.i384100.net/Probability\\n\\nOTHER RELATED COURSES (7 day free trial)\\nüìï ‚≠ê Deep Learning </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Specialization: https://imp.i384100.net/Deep-Learning\\nüìï Python for Everybody: https://imp.i384100.net/python\\nüìï </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">MLOps Course: https://imp.i384100.net/MLOps\\nüìï Natural Language Processing (NLP): https://imp.i384100.net/NLP\\nüìï </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Machine Learning in Production: https://imp.i384100.net/MLProduction\\nüìï Data Science Specialization: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://imp.i384100.net/DataScience\\nüìï Tensorflow: https://imp.i384100.net/Tensorflow\\n\\nREFERENCES\\n[1] The main </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Paper: https://arxiv.org/abs/1706.03762\\n[2] Tensor2Tensor has some code with a tutorial: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://www.tensorflow.org/tutorials/text/transformer\\n[3] Transformer very intuitively explained - Amazing: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">http://jalammar.github.io/illustrated-transformer/\\n[4] Medium Blog on intuitive explanation: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04\\n[5] Pretrained word embeddings: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://nlp.stanford.edu/projects/glove/\\n[6] Intuitive explanation of Layer normalization: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/\\n[7] Paper that gives </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">even better results than transformers (Pervasive Attention): https://arxiv.org/abs/1808.03867\\n[8] BERT uses </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transformers to pretrain neural nets for common NLP tasks. : </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\\n[9] Stanford Lecture on RNN: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture10.pdf\\n[10] Colah‚Äôs Blog: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://colah.github.io/posts/2015-08-Understanding-LSTMs/\\n[11] Wiki for timeseries of events: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'video_id'\u001b[0m: \u001b[32m'TQQlZhbC5ps'\u001b[0m,\n",
       "    \u001b[32m'title'\u001b[0m: \u001b[32m'Transformer Neural Networks - EXPLAINED! \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAttention is all you need\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'author'\u001b[0m: \u001b[32m'CodeEmporium'\u001b[0m,\n",
       "    \u001b[32m'keywords'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'Machine Learning'\u001b[0m,\n",
       "        \u001b[32m'Deep Learning'\u001b[0m,\n",
       "        \u001b[32m'Data Science'\u001b[0m,\n",
       "        \u001b[32m'Artificial Intelligence'\u001b[0m,\n",
       "        \u001b[32m'Neural Network'\u001b[0m,\n",
       "        \u001b[32m'attention'\u001b[0m,\n",
       "        \u001b[32m'attention is all you need'\u001b[0m,\n",
       "        \u001b[32m'attention neural networks'\u001b[0m,\n",
       "        \u001b[32m'transformer neural networks'\u001b[0m,\n",
       "        \u001b[32m'most important paper in deep learning'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'publish_date'\u001b[0m: \u001b[1;35mdatetime.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2020\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m13\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[33mtzinfo\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.timezone\u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mdatetime.timedelta\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdays\u001b[0m=\u001b[1;36m-1\u001b[0m, \n",
       "\u001b[33mseconds\u001b[0m=\u001b[1;36m57600\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'length'\u001b[0m: \u001b[1;36m785\u001b[0m,\n",
       "    \u001b[32m'likes'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'views'\u001b[0m: \u001b[1;36m810097\u001b[0m,\n",
       "    \u001b[32m'channel_id'\u001b[0m: \u001b[32m'UC5_6ZD6s8klmMu9TXEB_1IA'\u001b[0m,\n",
       "    \u001b[32m'thumbnail_url'\u001b[0m: \u001b[32m'https://i.ytimg.com/vi/TQQlZhbC5ps/sddefault.jpg'\u001b[0m,\n",
       "    \u001b[32m'description'\u001b[0m: \u001b[32m'Please subscribe to keep me alive: \u001b[0m\n",
       "\u001b[32mhttps://www.youtube.com/c/CodeEmporium?\u001b[0m\u001b[32msub_confirmation\u001b[0m\u001b[32m=\u001b[0m\u001b[32m1\u001b[0m\u001b[32m\\n\\nBLOG: https://medium.com/@dataemporium\\n\\n\\nPLAYLISTS \u001b[0m\n",
       "\u001b[32mFROM MY CHANNEL\\n‚≠ï Reinforcement Learning: \u001b[0m\n",
       "\u001b[32mhttps://youtube.com/playlist?\u001b[0m\u001b[32mlist\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPLTl9hO2Oobd9kS\u001b[0m\u001b[32m--NgVz0EPNyEmygV1Ha&\u001b[0m\u001b[32msi\u001b[0m\u001b[32m=\u001b[0m\u001b[32mAuThDZJwG19cgTA8\u001b[0m\u001b[32m\\nNatural Language \u001b[0m\n",
       "\u001b[32mProcessing: https://youtube.com/playlist?\u001b[0m\u001b[32mlist\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE\u001b[0m\u001b[32m&\u001b[0m\u001b[32msi\u001b[0m\u001b[32m=\u001b[0m\u001b[32mLsVy8RDPu8jeO\u001b[0m\u001b[32m-cc\\n‚≠ï \u001b[0m\n",
       "\u001b[32mTransformers from Scratch: https://youtube.com/playlist?\u001b[0m\u001b[32mlist\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE\u001b[0m\u001b[32m\\n‚≠ï ChatGPT \u001b[0m\n",
       "\u001b[32mPlaylist: https://youtube.com/playlist?\u001b[0m\u001b[32mlist\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPLTl9hO2Oobd9coYT6XsTraTBo4pL1j4HJ\u001b[0m\u001b[32m\\n‚≠ï  Convolutional Neural Networks: \u001b[0m\n",
       "\u001b[32mhttps://youtube.com/playlist?\u001b[0m\u001b[32mlist\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPLTl9hO2Oobd9U0XHz62Lw6EgIMkQpfz74\u001b[0m\u001b[32m\\n‚≠ï  The Math You Should Know : \u001b[0m\n",
       "\u001b[32mhttps://youtube.com/playlist?\u001b[0m\u001b[32mlist\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPLTl9hO2Oobd\u001b[0m\u001b[32m-_5sGLnbgE8Poer1Xjzz4h\\n‚≠ï Probability Theory for Machine Learning: \u001b[0m\n",
       "\u001b[32mhttps://youtube.com/playlist?\u001b[0m\u001b[32mlist\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPLTl9hO2Oobd9bPcq0fj91Jgk_\u001b[0m\u001b[32m-h1H_W3V\\n‚≠ï Coding Machine Learning: \u001b[0m\n",
       "\u001b[32mhttps://youtube.com/playlist?\u001b[0m\u001b[32mlist\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPLTl9hO2Oobd82vcsOnvCNzxrZOlrz3RiD\u001b[0m\u001b[32m\\n\\n\\nMATH COURSES \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7 day free trial\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nüìï \u001b[0m\n",
       "\u001b[32mMathematics for Machine Learning: https://imp.i384100.net/MathML\\nüìï Calculus: https://imp.i384100.net/Calculus\\nüìï\u001b[0m\n",
       "\u001b[32mStatistics for Data Science: https://imp.i384100.net/AdvancedStatistics\\nüìï Bayesian Statistics: \u001b[0m\n",
       "\u001b[32mhttps://imp.i384100.net/BayesianStatistics\\nüìï Linear Algebra: https://imp.i384100.net/LinearAlgebra\\nüìï \u001b[0m\n",
       "\u001b[32mProbability: https://imp.i384100.net/Probability\\n\\nOTHER RELATED COURSES \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7 day free trial\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nüìï ‚≠ê Deep Learning \u001b[0m\n",
       "\u001b[32mSpecialization: https://imp.i384100.net/Deep-Learning\\nüìï Python for Everybody: https://imp.i384100.net/python\\nüìï \u001b[0m\n",
       "\u001b[32mMLOps Course: https://imp.i384100.net/MLOps\\nüìï Natural Language Processing \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNLP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: https://imp.i384100.net/NLP\\nüìï \u001b[0m\n",
       "\u001b[32mMachine Learning in Production: https://imp.i384100.net/MLProduction\\nüìï Data Science Specialization: \u001b[0m\n",
       "\u001b[32mhttps://imp.i384100.net/DataScience\\nüìï Tensorflow: https://imp.i384100.net/Tensorflow\\n\\nREFERENCES\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The main \u001b[0m\n",
       "\u001b[32mPaper: https://arxiv.org/abs/1706.03762\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Tensor2Tensor has some code with a tutorial: \u001b[0m\n",
       "\u001b[32mhttps://www.tensorflow.org/tutorials/text/transformer\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Transformer very intuitively explained - Amazing: \u001b[0m\n",
       "\u001b[32mhttp://jalammar.github.io/illustrated-transformer/\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m4\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Medium Blog on intuitive explanation: \u001b[0m\n",
       "\u001b[32mhttps://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Pretrained word embeddings: \u001b[0m\n",
       "\u001b[32mhttps://nlp.stanford.edu/projects/glove/\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Intuitive explanation of Layer normalization: \u001b[0m\n",
       "\u001b[32mhttps://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m7\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Paper that gives \u001b[0m\n",
       "\u001b[32meven better results than transformers \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPervasive Attention\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: https://arxiv.org/abs/1808.03867\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m8\u001b[0m\u001b[32m]\u001b[0m\u001b[32m BERT uses \u001b[0m\n",
       "\u001b[32mtransformers to pretrain neural nets for common NLP tasks. : \u001b[0m\n",
       "\u001b[32mhttps://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Stanford Lecture on RNN: \u001b[0m\n",
       "\u001b[32mhttp://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture10.pdf\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m10\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Colah‚Äôs Blog: \u001b[0m\n",
       "\u001b[32mhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m11\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Wiki for timeseries of events: \u001b[0m\n",
       "\u001b[32mhttps://en.wikipedia.org/wiki/Transformer_\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmachine_learning_model\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich. print ( video_details )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "09d8d019-50e7-49a8-9a1f-6adbedce291b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript ,transcript_dict= get_transcript(video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41ae6860-a25c-4099-bf74-ceceac27e5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_time = get_transcript_time(video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "36334513-52d3-44a6-9d41-6621b186b7e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"recurrent neural Nets they are feed-forward neural networks rolled out over time as such they deal with sequence data where the input has some defined ordering this gives rise to several types of architectures the first is vector to sequence models these neural nets take in a fixed size vector as input and it outputs a sequence of any length in image captioning for example the input can be a vector representation of an image and the output sequence is a sentence that describes the image the second type is a sequence to vector model these neural networks taken a sequences input and spits out a fixed length vector in sentiment analysis the movie review is an input and a fixed size vector is the output indicating how good or bad this person thought the movie was sequence to sequence models is the more popular variant and these neural networks taken a sequences input and outputs another sequence so for example language translation the input could be a sentence in Spanish and the output is the translation in English do you have some time series data to model well our nen's would be the go-to however rnns have some problems our nuns are slow so slow that we use a truncated version of back propagation to Train it and even that's too Hardware intense and also they can't deal with long sequences very well we get gradients that vanish and explode if the network is too long in comes lsdm networks in 1991 that introduced a long short term memory cell in place of dumb neurons this cell has a branch that allows passed information to skip a lot of the processing of the current cell and move on to the next this allows the memory to be retained for longer sequences now to that second point we seem to be able to deal with longer sequences well or are we well kind of probably if the order of hundreds of words instead of a thousand words however to the first point normal our ends are but LS TMS are even slower they are more complex for these RN and LST M networks input data needs to be passed sequentially or serially one after the other we need inputs of the previous state to make any operations on the current state such sequential flow does not make use of today's GPUs very well which are designed for parallel computation so question how can we use parallelization for sequential data in 2017 the Transformer neural network architecture was introduced the network employs an encoder decoder architecture much like recurrent neural Nets the difference is that the input sequence can be passed in parallel consider translating a sentence from English to French I'll use this as a running example throughout the video with an RNN encoder we pass an input English sentence one word after the other the current words hidden state has dependencies in the previous words hidden state the word embeddings are generated one time step at a time with a transformer encoder on the other hand there is no concept of time step for the input we pass in all the words of the sentence simultaneously and determine the word embeddings simultaneously so how is it doing this let's pick a part the transformer architecture I'll make multiple passes on the explanation in the first pass will be like a high overview and the next rounds we'll get into more details let's start with input embeddings computers don't understand words they get numbers they get vectors and matrices the idea is to map every word to a point in space where similar words in meaning are physically closer to each other the space in which they are present is called an embedding space we could pre train this embedding space to save time or even just use an already pre trained embedding space this embedding space Maps a word to a vector but the same word in different sentences may have different meanings this is where positional encoders come in it's a vector that has information on distances between words and the sentence the original paper uses a sine and cosine function to generate this vector but it could be any reasonable function after passing the English sentence through the input embedding and applying the positional encoding we get word vectors that have positional information that is context nice we pass this in to the encoder block where it goes through a multi-headed attention layer and a feed-forward layer okay one at a time attention it involves answering what part of the input should I focus on if we are translating from English to French and we are doing self attention that is attention with respect to oneself the question we want to answer is how relevant is the ithe word in the English sentence relevant to other words in the same English sentence this is represented in the I thought ention vector and it is computed in the attention block for every word we can have an attention vector generated which captures contextual relationships between words in the sentence so that's great the other important unit is a feed-forward net this is just a simple feed-forward neural network that is applied to every one of the attention vectors these feed-forward nets are used in practice to transform the attention vectors into a form that is digestible by the next encoder block or decoder block now that's the high-level overview of the encoder components let's talk about the decoder now during the training phase for English to French we feed the output French sentence to the decoder but remember computers don't get language they get numbers vectors and matrices so we process it using the input embedding to get the vector form of the word and then we add a positional vector to get the notion of context of the word in a sentence we pass this vector finally into a decoder block that has three main components two of which are similar to the encoder block the self attention block generates attention vectors for every word in the french sentence to represent how much each word is related to every word in the same sentence these attention vectors and vectors from the encoder are passed into another attention block let's call this the encoder decoder attention block since we have one vector from every word in the English and French sentences this attention block will determine how related each word vector is with respect to each other and this is where the main English to French word mapping happens the output of this block is attention vectors for every word in English and the French sentence each vector representing the relationships with other words in both the languages next we pass each attention vector to a feed-forward unit this makes the output vector more digestible by the next decoder block or a linear layer now the linear layer is surprise-surprise another feed for connected layer it's used to expand the dimensions into the number of words in the french language the softmax layer transforms it into a probability distribution which is now human interpretable and the final word is the word corresponding to the highest probability overall this decoder predicts the next word and we execute this over multiple time steps until the end of sentence token is generated that's our first passed over the explanation of the entire network architecture for transformers but let's go over it again but this time introduce even more details going deeper an input English sentence is converted into an embedding to represent meaning we add a positional vector to get the context of the word in the sentence our attention block computes the attention vectors for each word only problem here is that the attention vector may not be too strong for every word the attention vector may weight its relation with itself much higher it's true but it's useless we are more interested in interactions with different words and so we determine like eight such attention vectors per word and take a weighted average to compute the final attention vector for every word since we use multiple attention vectors we call it the multi-head attention block the attention vectors are passed in through a feed-forward net one vector at a time the cool thing is that each of the attention nets are independent of each other so we can use some beautiful parallelization here because of this we can pass all our words at the same time into the encoder block and the output will be a set of encoded vectors for every word now the decoder we first obtained the embedding of French words to encode meaning then add the positional value to retain context they are then passed to the first attention block the paper calls this the masked attention block why is this the case though it's because while generating the next French word we can use all the words from the English sentence but only the previous words of the French sentence if we are going to use all the words in the French sentence then there would be no learning it would just spit out the next word so while performing parallelization with matrix operations we make sure that the matrix will mask the words appearing later by transforming it into zeros so the attention network can't use them the next detention block which is the encoder decoder attention block generates similar attention vectors for every English and French word these are passed into the feed-forward layer linear layer and the softmax layer to predict the next word that's the past 2 over the architecture explained I hope you're understanding more and more details here now for the next pass where we go even deeper how exactly do these multi-head attention networks look now the single headed attention looks like this QK and V are abstract vectors that extract different components of an input word we have QK and V vectors for every single word we use these to compute the attention vectors for every word using this kind of formula for a multi-headed attention we have multiple weight matrices you qwk and WV so we will have multiple attention vectors Z for every word however our neural net is only expecting one attention vector per word so we use another weighted matrix wz to make sure that the output is still an attention vector per word additionally after every layer we apply some form of normalization typically we would apply a patch normalization this movements out the law surface making it easier to optimize while using larger learning rates this is the TLDR but that's what it does but we can actually use something called layer normalization making the normalization across each feature instead of each sample it's better for stabilization if you are interested in dabbling in transformer code tensorflow has a step-by-step tutorial that can get you up to speed transformer neural nets have largely replaced LS TM nets for sequence to vector sequence to sequence and vector to sequence problems Google for example created Bert which uses transformers to pre train models for common NLP tasks read that blog it's good however there was another paper called pervasive attention that could be even better than transformers for sequence to sequence models although transformers can be better suited for a wider variety of problems it's still a very interesting read I'll link it in the description below with other resources so check that out hope this helped you get you up to speed with transformer neural nets if you liked the video hit that like button subscribe to stay up to date with some deep learning and machine learning knowledge and I will see you guys in the next one bye bye\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "370fa2b2-2724-4e0d-b70c-8b14d545aec9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'recurrent neural Nets they are', 'start': 0.03, 'duration': 4.289},\n",
       " {'text': 'feed-forward neural networks rolled out',\n",
       "  'start': 2.399,\n",
       "  'duration': 4.38},\n",
       " {'text': 'over time as such they deal with',\n",
       "  'start': 4.319,\n",
       "  'duration': 4.981},\n",
       " {'text': 'sequence data where the input has some',\n",
       "  'start': 6.779,\n",
       "  'duration': 4.86},\n",
       " {'text': 'defined ordering this gives rise to',\n",
       "  'start': 9.3,\n",
       "  'duration': 5.49},\n",
       " {'text': 'several types of architectures the first',\n",
       "  'start': 11.639,\n",
       "  'duration': 5.941},\n",
       " {'text': 'is vector to sequence models these',\n",
       "  'start': 14.79,\n",
       "  'duration': 3.51},\n",
       " {'text': 'neural nets', 'start': 17.58, 'duration': 3.09},\n",
       " {'text': 'take in a fixed size vector as input and',\n",
       "  'start': 18.3,\n",
       "  'duration': 5.37},\n",
       " {'text': 'it outputs a sequence of any length in',\n",
       "  'start': 20.67,\n",
       "  'duration': 5.939},\n",
       " {'text': 'image captioning for example the input',\n",
       "  'start': 23.67,\n",
       "  'duration': 4.65},\n",
       " {'text': 'can be a vector representation of an',\n",
       "  'start': 26.609,\n",
       "  'duration': 4.41},\n",
       " {'text': 'image and the output sequence is a',\n",
       "  'start': 28.32,\n",
       "  'duration': 5.7},\n",
       " {'text': 'sentence that describes the image the',\n",
       "  'start': 31.019,\n",
       "  'duration': 4.861},\n",
       " {'text': 'second type is a sequence to vector',\n",
       "  'start': 34.02,\n",
       "  'duration': 4.62},\n",
       " {'text': 'model these neural networks taken a',\n",
       "  'start': 35.88,\n",
       "  'duration': 4.56},\n",
       " {'text': 'sequences input and spits out a fixed',\n",
       "  'start': 38.64,\n",
       "  'duration': 4.98},\n",
       " {'text': 'length vector in sentiment analysis the',\n",
       "  'start': 40.44,\n",
       "  'duration': 5.549},\n",
       " {'text': 'movie review is an input and a fixed',\n",
       "  'start': 43.62,\n",
       "  'duration': 4.65},\n",
       " {'text': 'size vector is the output indicating how',\n",
       "  'start': 45.989,\n",
       "  'duration': 4.89},\n",
       " {'text': 'good or bad this person thought the',\n",
       "  'start': 48.27,\n",
       "  'duration': 6.39},\n",
       " {'text': 'movie was sequence to sequence models is',\n",
       "  'start': 50.879,\n",
       "  'duration': 6.391},\n",
       " {'text': 'the more popular variant and these',\n",
       "  'start': 54.66,\n",
       "  'duration': 4.71},\n",
       " {'text': 'neural networks taken a sequences input',\n",
       "  'start': 57.27,\n",
       "  'duration': 5.1},\n",
       " {'text': 'and outputs another sequence so for',\n",
       "  'start': 59.37,\n",
       "  'duration': 5.28},\n",
       " {'text': 'example language translation the input',\n",
       "  'start': 62.37,\n",
       "  'duration': 4.499},\n",
       " {'text': 'could be a sentence in Spanish and the',\n",
       "  'start': 64.65,\n",
       "  'duration': 6.99},\n",
       " {'text': 'output is the translation in English do',\n",
       "  'start': 66.869,\n",
       "  'duration': 6.391},\n",
       " {'text': 'you have some time series data to model',\n",
       "  'start': 71.64,\n",
       "  'duration': 3.35},\n",
       " {'text': \"well our nen's would be the go-to\",\n",
       "  'start': 73.26,\n",
       "  'duration': 5.94},\n",
       " {'text': 'however rnns have some problems our nuns',\n",
       "  'start': 74.99,\n",
       "  'duration': 7.69},\n",
       " {'text': 'are slow so slow that we use a truncated',\n",
       "  'start': 79.2,\n",
       "  'duration': 5.52},\n",
       " {'text': 'version of back propagation to Train it',\n",
       "  'start': 82.68,\n",
       "  'duration': 5.149},\n",
       " {'text': \"and even that's too Hardware intense and\",\n",
       "  'start': 84.72,\n",
       "  'duration': 5.91},\n",
       " {'text': \"also they can't deal with long sequences\",\n",
       "  'start': 87.829,\n",
       "  'duration': 5.231},\n",
       " {'text': 'very well we get gradients that vanish',\n",
       "  'start': 90.63,\n",
       "  'duration': 4.91},\n",
       " {'text': 'and explode if the network is too long',\n",
       "  'start': 93.06,\n",
       "  'duration': 5.879},\n",
       " {'text': 'in comes lsdm networks in 1991 that',\n",
       "  'start': 95.54,\n",
       "  'duration': 6.07},\n",
       " {'text': 'introduced a long short term memory cell',\n",
       "  'start': 98.939,\n",
       "  'duration': 6.72},\n",
       " {'text': 'in place of dumb neurons this cell has a',\n",
       "  'start': 101.61,\n",
       "  'duration': 6.719},\n",
       " {'text': 'branch that allows passed information to',\n",
       "  'start': 105.659,\n",
       "  'duration': 4.5},\n",
       " {'text': 'skip a lot of the processing of the',\n",
       "  'start': 108.329,\n",
       "  'duration': 3.72},\n",
       " {'text': 'current cell and move on to the next',\n",
       "  'start': 110.159,\n",
       "  'duration': 4.471},\n",
       " {'text': 'this allows the memory to be retained',\n",
       "  'start': 112.049,\n",
       "  'duration': 5.82},\n",
       " {'text': 'for longer sequences now to that second',\n",
       "  'start': 114.63,\n",
       "  'duration': 5.01},\n",
       " {'text': 'point we seem to be able to deal with',\n",
       "  'start': 117.869,\n",
       "  'duration': 6.451},\n",
       " {'text': 'longer sequences well or are we well',\n",
       "  'start': 119.64,\n",
       "  'duration': 6.9},\n",
       " {'text': 'kind of probably if the order of',\n",
       "  'start': 124.32,\n",
       "  'duration': 4.32},\n",
       " {'text': 'hundreds of words instead of a thousand',\n",
       "  'start': 126.54,\n",
       "  'duration': 5.88},\n",
       " {'text': 'words however to the first point normal',\n",
       "  'start': 128.64,\n",
       "  'duration': 4.959},\n",
       " {'text': 'our ends are', 'start': 132.42, 'duration': 4.959},\n",
       " {'text': 'but LS TMS are even slower they are more',\n",
       "  'start': 133.599,\n",
       "  'duration': 7.29},\n",
       " {'text': 'complex for these RN and LST M networks',\n",
       "  'start': 137.379,\n",
       "  'duration': 5.22},\n",
       " {'text': 'input data needs to be passed', 'start': 140.889, 'duration': 4.44},\n",
       " {'text': 'sequentially or serially one after the',\n",
       "  'start': 142.599,\n",
       "  'duration': 3.0},\n",
       " {'text': 'other', 'start': 145.329, 'duration': 2.97},\n",
       " {'text': 'we need inputs of the previous state to',\n",
       "  'start': 145.599,\n",
       "  'duration': 5.39},\n",
       " {'text': 'make any operations on the current state',\n",
       "  'start': 148.299,\n",
       "  'duration': 5.28},\n",
       " {'text': 'such sequential flow does not make use',\n",
       "  'start': 150.989,\n",
       "  'duration': 5.14},\n",
       " {'text': \"of today's GPUs very well which are\",\n",
       "  'start': 153.579,\n",
       "  'duration': 6.151},\n",
       " {'text': 'designed for parallel computation so',\n",
       "  'start': 156.129,\n",
       "  'duration': 6.151},\n",
       " {'text': 'question how can we use parallelization',\n",
       "  'start': 159.73,\n",
       "  'duration': 6.569},\n",
       " {'text': 'for sequential data in 2017 the',\n",
       "  'start': 162.28,\n",
       "  'duration': 5.67},\n",
       " {'text': 'Transformer neural network architecture',\n",
       "  'start': 166.299,\n",
       "  'duration': 4.77},\n",
       " {'text': 'was introduced the network employs an',\n",
       "  'start': 167.95,\n",
       "  'duration': 5.34},\n",
       " {'text': 'encoder decoder architecture much like',\n",
       "  'start': 171.069,\n",
       "  'duration': 5.521},\n",
       " {'text': 'recurrent neural Nets the difference is',\n",
       "  'start': 173.29,\n",
       "  'duration': 5.61},\n",
       " {'text': 'that the input sequence can be passed in',\n",
       "  'start': 176.59,\n",
       "  'duration': 5.519},\n",
       " {'text': 'parallel consider translating a sentence',\n",
       "  'start': 178.9,\n",
       "  'duration': 5.399},\n",
       " {'text': \"from English to French I'll use this as\",\n",
       "  'start': 182.109,\n",
       "  'duration': 4.1},\n",
       " {'text': 'a running example throughout the video',\n",
       "  'start': 184.299,\n",
       "  'duration': 5.19},\n",
       " {'text': 'with an RNN encoder we pass an input',\n",
       "  'start': 186.209,\n",
       "  'duration': 5.14},\n",
       " {'text': 'English sentence one word after the',\n",
       "  'start': 189.489,\n",
       "  'duration': 4.351},\n",
       " {'text': 'other the current words hidden state has',\n",
       "  'start': 191.349,\n",
       "  'duration': 4.26},\n",
       " {'text': 'dependencies in the previous words',\n",
       "  'start': 193.84,\n",
       "  'duration': 3.929},\n",
       " {'text': 'hidden state the word embeddings are',\n",
       "  'start': 195.609,\n",
       "  'duration': 6.06},\n",
       " {'text': 'generated one time step at a time with a',\n",
       "  'start': 197.769,\n",
       "  'duration': 5.791},\n",
       " {'text': 'transformer encoder on the other hand',\n",
       "  'start': 201.669,\n",
       "  'duration': 4.74},\n",
       " {'text': 'there is no concept of time step for the',\n",
       "  'start': 203.56,\n",
       "  'duration': 4.949},\n",
       " {'text': 'input we pass in all the words of the',\n",
       "  'start': 206.409,\n",
       "  'duration': 3.871},\n",
       " {'text': 'sentence simultaneously and determine',\n",
       "  'start': 208.509,\n",
       "  'duration': 4.801},\n",
       " {'text': 'the word embeddings simultaneously so',\n",
       "  'start': 210.28,\n",
       "  'duration': 6.09},\n",
       " {'text': \"how is it doing this let's pick a part\",\n",
       "  'start': 213.31,\n",
       "  'duration': 5.129},\n",
       " {'text': \"the transformer architecture I'll make\",\n",
       "  'start': 216.37,\n",
       "  'duration': 4.079},\n",
       " {'text': 'multiple passes on the explanation in',\n",
       "  'start': 218.439,\n",
       "  'duration': 3.33},\n",
       " {'text': 'the first pass will be like a high',\n",
       "  'start': 220.449,\n",
       "  'duration': 3.03},\n",
       " {'text': \"overview and the next rounds we'll get\",\n",
       "  'start': 221.769,\n",
       "  'duration': 5.101},\n",
       " {'text': \"into more details let's start with input\",\n",
       "  'start': 223.479,\n",
       "  'duration': 5.76},\n",
       " {'text': \"embeddings computers don't understand\",\n",
       "  'start': 226.87,\n",
       "  'duration': 4.709},\n",
       " {'text': 'words they get numbers they get vectors',\n",
       "  'start': 229.239,\n",
       "  'duration': 3.33},\n",
       " {'text': 'and matrices', 'start': 231.579, 'duration': 3.421},\n",
       " {'text': 'the idea is to map every word to a point',\n",
       "  'start': 232.569,\n",
       "  'duration': 4.32},\n",
       " {'text': 'in space where similar words in meaning',\n",
       "  'start': 235.0,\n",
       "  'duration': 4.319},\n",
       " {'text': 'are physically closer to each other the',\n",
       "  'start': 236.889,\n",
       "  'duration': 3.84},\n",
       " {'text': 'space in which they are present is',\n",
       "  'start': 239.319,\n",
       "  'duration': 3.931},\n",
       " {'text': 'called an embedding space we could pre',\n",
       "  'start': 240.729,\n",
       "  'duration': 4.26},\n",
       " {'text': 'train this embedding space to save time',\n",
       "  'start': 243.25,\n",
       "  'duration': 4.229},\n",
       " {'text': 'or even just use an already pre trained',\n",
       "  'start': 244.989,\n",
       "  'duration': 5.071},\n",
       " {'text': 'embedding space this embedding space',\n",
       "  'start': 247.479,\n",
       "  'duration': 5.551},\n",
       " {'text': 'Maps a word to a vector but the same',\n",
       "  'start': 250.06,\n",
       "  'duration': 4.979},\n",
       " {'text': 'word in different sentences may have',\n",
       "  'start': 253.03,\n",
       "  'duration': 4.29},\n",
       " {'text': 'different meanings this is where',\n",
       "  'start': 255.039,\n",
       "  'duration': 5.07},\n",
       " {'text': \"positional encoders come in it's a\",\n",
       "  'start': 257.32,\n",
       "  'duration': 5.129},\n",
       " {'text': 'vector that has information on distances',\n",
       "  'start': 260.109,\n",
       "  'duration': 4.711},\n",
       " {'text': 'between words and the sentence the',\n",
       "  'start': 262.449,\n",
       "  'duration': 4.711},\n",
       " {'text': 'original paper uses a sine and cosine',\n",
       "  'start': 264.82,\n",
       "  'duration': 4.92},\n",
       " {'text': 'function to generate this vector but it',\n",
       "  'start': 267.16,\n",
       "  'duration': 5.34},\n",
       " {'text': 'could be any reasonable function after',\n",
       "  'start': 269.74,\n",
       "  'duration': 4.65},\n",
       " {'text': 'passing the English sentence through the',\n",
       "  'start': 272.5,\n",
       "  'duration': 4.02},\n",
       " {'text': 'input embedding and applying the',\n",
       "  'start': 274.39,\n",
       "  'duration': 4.98},\n",
       " {'text': 'positional encoding we get word vectors',\n",
       "  'start': 276.52,\n",
       "  'duration': 5.31},\n",
       " {'text': 'that have positional information that is',\n",
       "  'start': 279.37,\n",
       "  'duration': 7.14},\n",
       " {'text': 'context nice we pass this in to the',\n",
       "  'start': 281.83,\n",
       "  'duration': 6.84},\n",
       " {'text': 'encoder block where it goes through a',\n",
       "  'start': 286.51,\n",
       "  'duration': 4.56},\n",
       " {'text': 'multi-headed attention layer and a',\n",
       "  'start': 288.67,\n",
       "  'duration': 5.72},\n",
       " {'text': 'feed-forward layer okay one at a time',\n",
       "  'start': 291.07,\n",
       "  'duration': 6.78},\n",
       " {'text': 'attention it involves answering what',\n",
       "  'start': 294.39,\n",
       "  'duration': 5.59},\n",
       " {'text': 'part of the input should I focus on if',\n",
       "  'start': 297.85,\n",
       "  'duration': 4.38},\n",
       " {'text': 'we are translating from English to',\n",
       "  'start': 299.98,\n",
       "  'duration': 4.98},\n",
       " {'text': 'French and we are doing self attention',\n",
       "  'start': 302.23,\n",
       "  'duration': 4.83},\n",
       " {'text': 'that is attention with respect to',\n",
       "  'start': 304.96,\n",
       "  'duration': 4.77},\n",
       " {'text': 'oneself the question we want to answer',\n",
       "  'start': 307.06,\n",
       "  'duration': 6.66},\n",
       " {'text': 'is how relevant is the ithe word in the',\n",
       "  'start': 309.73,\n",
       "  'duration': 6.57},\n",
       " {'text': 'English sentence relevant to other words',\n",
       "  'start': 313.72,\n",
       "  'duration': 6.12},\n",
       " {'text': 'in the same English sentence this is',\n",
       "  'start': 316.3,\n",
       "  'duration': 5.82},\n",
       " {'text': 'represented in the I thought ention',\n",
       "  'start': 319.84,\n",
       "  'duration': 4.38},\n",
       " {'text': 'vector and it is computed in the',\n",
       "  'start': 322.12,\n",
       "  'duration': 5.4},\n",
       " {'text': 'attention block for every word we can',\n",
       "  'start': 324.22,\n",
       "  'duration': 5.34},\n",
       " {'text': 'have an attention vector generated which',\n",
       "  'start': 327.52,\n",
       "  'duration': 4.14},\n",
       " {'text': 'captures contextual relationships',\n",
       "  'start': 329.56,\n",
       "  'duration': 5.16},\n",
       " {'text': \"between words in the sentence so that's\",\n",
       "  'start': 331.66,\n",
       "  'duration': 3.65},\n",
       " {'text': 'great', 'start': 334.72, 'duration': 3.12},\n",
       " {'text': 'the other important unit is a', 'start': 335.31, 'duration': 5.62},\n",
       " {'text': 'feed-forward net this is just a simple',\n",
       "  'start': 337.84,\n",
       "  'duration': 4.74},\n",
       " {'text': 'feed-forward neural network that is',\n",
       "  'start': 340.93,\n",
       "  'duration': 3.3},\n",
       " {'text': 'applied to every one of the attention',\n",
       "  'start': 342.58,\n",
       "  'duration': 4.56},\n",
       " {'text': 'vectors these feed-forward nets are used',\n",
       "  'start': 344.23,\n",
       "  'duration': 5.01},\n",
       " {'text': 'in practice to transform the attention',\n",
       "  'start': 347.14,\n",
       "  'duration': 4.38},\n",
       " {'text': 'vectors into a form that is digestible',\n",
       "  'start': 349.24,\n",
       "  'duration': 4.62},\n",
       " {'text': 'by the next encoder block or decoder',\n",
       "  'start': 351.52,\n",
       "  'duration': 5.43},\n",
       " {'text': \"block now that's the high-level overview\",\n",
       "  'start': 353.86,\n",
       "  'duration': 6.15},\n",
       " {'text': \"of the encoder components let's talk\",\n",
       "  'start': 356.95,\n",
       "  'duration': 6.39},\n",
       " {'text': 'about the decoder now during the',\n",
       "  'start': 360.01,\n",
       "  'duration': 5.28},\n",
       " {'text': 'training phase for English to French we',\n",
       "  'start': 363.34,\n",
       "  'duration': 4.44},\n",
       " {'text': 'feed the output French sentence to the',\n",
       "  'start': 365.29,\n",
       "  'duration': 5.64},\n",
       " {'text': \"decoder but remember computers don't get\",\n",
       "  'start': 367.78,\n",
       "  'duration': 5.52},\n",
       " {'text': 'language they get numbers vectors and',\n",
       "  'start': 370.93,\n",
       "  'duration': 4.98},\n",
       " {'text': 'matrices so we process it using the',\n",
       "  'start': 373.3,\n",
       "  'duration': 4.65},\n",
       " {'text': 'input embedding to get the vector form',\n",
       "  'start': 375.91,\n",
       "  'duration': 4.71},\n",
       " {'text': 'of the word and then we add a positional',\n",
       "  'start': 377.95,\n",
       "  'duration': 5.1},\n",
       " {'text': 'vector to get the notion of context of',\n",
       "  'start': 380.62,\n",
       "  'duration': 5.58},\n",
       " {'text': 'the word in a sentence we pass this',\n",
       "  'start': 383.05,\n",
       "  'duration': 6.66},\n",
       " {'text': 'vector finally into a decoder block that',\n",
       "  'start': 386.2,\n",
       "  'duration': 5.7},\n",
       " {'text': 'has three main components two of which',\n",
       "  'start': 389.71,\n",
       "  'duration': 5.07},\n",
       " {'text': 'are similar to the encoder block the',\n",
       "  'start': 391.9,\n",
       "  'duration': 4.95},\n",
       " {'text': 'self attention block generates attention',\n",
       "  'start': 394.78,\n",
       "  'duration': 3.99},\n",
       " {'text': 'vectors for every word in the french',\n",
       "  'start': 396.85,\n",
       "  'duration': 4.26},\n",
       " {'text': 'sentence to represent how much', 'start': 398.77, 'duration': 4.41},\n",
       " {'text': 'each word is related to every word in',\n",
       "  'start': 401.11,\n",
       "  'duration': 4.53},\n",
       " {'text': 'the same sentence these attention',\n",
       "  'start': 403.18,\n",
       "  'duration': 4.92},\n",
       " {'text': 'vectors and vectors from the encoder are',\n",
       "  'start': 405.64,\n",
       "  'duration': 4.74},\n",
       " {'text': 'passed into another attention block',\n",
       "  'start': 408.1,\n",
       "  'duration': 4.59},\n",
       " {'text': \"let's call this the encoder decoder\",\n",
       "  'start': 410.38,\n",
       "  'duration': 4.89},\n",
       " {'text': 'attention block since we have one vector',\n",
       "  'start': 412.69,\n",
       "  'duration': 4.92},\n",
       " {'text': 'from every word in the English and',\n",
       "  'start': 415.27,\n",
       "  'duration': 6.27},\n",
       " {'text': 'French sentences this attention block',\n",
       "  'start': 417.61,\n",
       "  'duration': 6.78},\n",
       " {'text': 'will determine how related each word',\n",
       "  'start': 421.54,\n",
       "  'duration': 5.01},\n",
       " {'text': 'vector is with respect to each other and',\n",
       "  'start': 424.39,\n",
       "  'duration': 4.26},\n",
       " {'text': 'this is where the main English to French',\n",
       "  'start': 426.55,\n",
       "  'duration': 5.25},\n",
       " {'text': 'word mapping happens the output of this',\n",
       "  'start': 428.65,\n",
       "  'duration': 5.76},\n",
       " {'text': 'block is attention vectors for every',\n",
       "  'start': 431.8,\n",
       "  'duration': 4.94},\n",
       " {'text': 'word in English and the French sentence',\n",
       "  'start': 434.41,\n",
       "  'duration': 4.59},\n",
       " {'text': 'each vector representing the', 'start': 436.74, 'duration': 4.54},\n",
       " {'text': 'relationships with other words in both',\n",
       "  'start': 439.0,\n",
       "  'duration': 5.79},\n",
       " {'text': 'the languages next we pass each',\n",
       "  'start': 441.28,\n",
       "  'duration': 6.27},\n",
       " {'text': 'attention vector to a feed-forward unit',\n",
       "  'start': 444.79,\n",
       "  'duration': 4.89},\n",
       " {'text': 'this makes the output vector more',\n",
       "  'start': 447.55,\n",
       "  'duration': 4.83},\n",
       " {'text': 'digestible by the next decoder block or',\n",
       "  'start': 449.68,\n",
       "  'duration': 6.36},\n",
       " {'text': 'a linear layer now the linear layer is',\n",
       "  'start': 452.38,\n",
       "  'duration': 4.89},\n",
       " {'text': 'surprise-surprise', 'start': 456.04, 'duration': 4.29},\n",
       " {'text': \"another feed for connected layer it's\",\n",
       "  'start': 457.27,\n",
       "  'duration': 5.07},\n",
       " {'text': 'used to expand the dimensions into the',\n",
       "  'start': 460.33,\n",
       "  'duration': 3.33},\n",
       " {'text': 'number of words in the french language',\n",
       "  'start': 462.34,\n",
       "  'duration': 4.08},\n",
       " {'text': 'the softmax layer transforms it into a',\n",
       "  'start': 463.66,\n",
       "  'duration': 5.25},\n",
       " {'text': 'probability distribution which is now',\n",
       "  'start': 466.42,\n",
       "  'duration': 4.98},\n",
       " {'text': 'human interpretable and the final word',\n",
       "  'start': 468.91,\n",
       "  'duration': 4.29},\n",
       " {'text': 'is the word corresponding to the highest',\n",
       "  'start': 471.4,\n",
       "  'duration': 4.38},\n",
       " {'text': 'probability overall this decoder',\n",
       "  'start': 473.2,\n",
       "  'duration': 5.19},\n",
       " {'text': 'predicts the next word and we execute',\n",
       "  'start': 475.78,\n",
       "  'duration': 4.8},\n",
       " {'text': 'this over multiple time steps until the',\n",
       "  'start': 478.39,\n",
       "  'duration': 4.97},\n",
       " {'text': 'end of sentence token is generated',\n",
       "  'start': 480.58,\n",
       "  'duration': 4.95},\n",
       " {'text': \"that's our first passed over the\",\n",
       "  'start': 483.36,\n",
       "  'duration': 4.15},\n",
       " {'text': 'explanation of the entire network',\n",
       "  'start': 485.53,\n",
       "  'duration': 4.98},\n",
       " {'text': \"architecture for transformers but let's\",\n",
       "  'start': 487.51,\n",
       "  'duration': 4.98},\n",
       " {'text': 'go over it again but this time introduce',\n",
       "  'start': 490.51,\n",
       "  'duration': 6.69},\n",
       " {'text': 'even more details going deeper an input',\n",
       "  'start': 492.49,\n",
       "  'duration': 6.78},\n",
       " {'text': 'English sentence is converted into an',\n",
       "  'start': 497.2,\n",
       "  'duration': 5.1},\n",
       " {'text': 'embedding to represent meaning we add a',\n",
       "  'start': 499.27,\n",
       "  'duration': 5.22},\n",
       " {'text': 'positional vector to get the context of',\n",
       "  'start': 502.3,\n",
       "  'duration': 4.89},\n",
       " {'text': 'the word in the sentence our attention',\n",
       "  'start': 504.49,\n",
       "  'duration': 4.89},\n",
       " {'text': 'block computes the attention vectors for',\n",
       "  'start': 507.19,\n",
       "  'duration': 5.4},\n",
       " {'text': 'each word only problem here is that the',\n",
       "  'start': 509.38,\n",
       "  'duration': 5.45},\n",
       " {'text': 'attention vector may not be too strong',\n",
       "  'start': 512.59,\n",
       "  'duration': 5.13},\n",
       " {'text': 'for every word the attention vector may',\n",
       "  'start': 514.83,\n",
       "  'duration': 4.899},\n",
       " {'text': 'weight its relation with itself much',\n",
       "  'start': 517.72,\n",
       "  'duration': 5.009},\n",
       " {'text': \"higher it's true but it's useless we are\",\n",
       "  'start': 519.729,\n",
       "  'duration': 4.591},\n",
       " {'text': 'more interested in interactions with',\n",
       "  'start': 522.729,\n",
       "  'duration': 4.141},\n",
       " {'text': 'different words and so we determine like',\n",
       "  'start': 524.32,\n",
       "  'duration': 4.74},\n",
       " {'text': 'eight such attention vectors per word',\n",
       "  'start': 526.87,\n",
       "  'duration': 4.74},\n",
       " {'text': 'and take a weighted average to compute',\n",
       "  'start': 529.06,\n",
       "  'duration': 4.86},\n",
       " {'text': 'the final attention vector for every',\n",
       "  'start': 531.61,\n",
       "  'duration': 3.229},\n",
       " {'text': 'word', 'start': 533.92, 'duration': 3.479},\n",
       " {'text': 'since we use multiple attention vectors',\n",
       "  'start': 534.839,\n",
       "  'duration': 4.24},\n",
       " {'text': 'we call it the multi-head attention',\n",
       "  'start': 537.399,\n",
       "  'duration': 4.411},\n",
       " {'text': 'block the attention vectors are passed',\n",
       "  'start': 539.079,\n",
       "  'duration': 5.341},\n",
       " {'text': 'in through a feed-forward net one vector',\n",
       "  'start': 541.81,\n",
       "  'duration': 5.25},\n",
       " {'text': 'at a time the cool thing is that each of',\n",
       "  'start': 544.42,\n",
       "  'duration': 4.349},\n",
       " {'text': 'the attention nets are independent of',\n",
       "  'start': 547.06,\n",
       "  'duration': 3.87},\n",
       " {'text': 'each other so we can use some beautiful',\n",
       "  'start': 548.769,\n",
       "  'duration': 5.25},\n",
       " {'text': 'parallelization here because of this we',\n",
       "  'start': 550.93,\n",
       "  'duration': 5.31},\n",
       " {'text': 'can pass all our words at the same time',\n",
       "  'start': 554.019,\n",
       "  'duration': 4.11},\n",
       " {'text': 'into the encoder block and the output',\n",
       "  'start': 556.24,\n",
       "  'duration': 3.899},\n",
       " {'text': 'will be a set of encoded vectors for',\n",
       "  'start': 558.129,\n",
       "  'duration': 3.26},\n",
       " {'text': 'every word', 'start': 560.139, 'duration': 4.26},\n",
       " {'text': 'now the decoder we first obtained the',\n",
       "  'start': 561.389,\n",
       "  'duration': 5.591},\n",
       " {'text': 'embedding of French words to encode',\n",
       "  'start': 564.399,\n",
       "  'duration': 5.94},\n",
       " {'text': 'meaning then add the positional value to',\n",
       "  'start': 566.98,\n",
       "  'duration': 6.959},\n",
       " {'text': 'retain context they are then passed to',\n",
       "  'start': 570.339,\n",
       "  'duration': 5.821},\n",
       " {'text': 'the first attention block the paper',\n",
       "  'start': 573.939,\n",
       "  'duration': 4.46},\n",
       " {'text': 'calls this the masked attention block',\n",
       "  'start': 576.16,\n",
       "  'duration': 5.19},\n",
       " {'text': \"why is this the case though it's because\",\n",
       "  'start': 578.399,\n",
       "  'duration': 5.141},\n",
       " {'text': 'while generating the next French word we',\n",
       "  'start': 581.35,\n",
       "  'duration': 4.08},\n",
       " {'text': 'can use all the words from the English',\n",
       "  'start': 583.54,\n",
       "  'duration': 4.68},\n",
       " {'text': 'sentence but only the previous words of',\n",
       "  'start': 585.43,\n",
       "  'duration': 5.159},\n",
       " {'text': 'the French sentence if we are going to',\n",
       "  'start': 588.22,\n",
       "  'duration': 3.869},\n",
       " {'text': 'use all the words in the French sentence',\n",
       "  'start': 590.589,\n",
       "  'duration': 3.841},\n",
       " {'text': 'then there would be no learning it would',\n",
       "  'start': 592.089,\n",
       "  'duration': 5.55},\n",
       " {'text': 'just spit out the next word so while',\n",
       "  'start': 594.43,\n",
       "  'duration': 5.04},\n",
       " {'text': 'performing parallelization with matrix',\n",
       "  'start': 597.639,\n",
       "  'duration': 4.111},\n",
       " {'text': 'operations we make sure that the matrix',\n",
       "  'start': 599.47,\n",
       "  'duration': 5.549},\n",
       " {'text': 'will mask the words appearing later by',\n",
       "  'start': 601.75,\n",
       "  'duration': 5.61},\n",
       " {'text': 'transforming it into zeros so the',\n",
       "  'start': 605.019,\n",
       "  'duration': 7.081},\n",
       " {'text': \"attention network can't use them the\",\n",
       "  'start': 607.36,\n",
       "  'duration': 6.539},\n",
       " {'text': 'next detention block which is the',\n",
       "  'start': 612.1,\n",
       "  'duration': 3.719},\n",
       " {'text': 'encoder decoder attention block',\n",
       "  'start': 613.899,\n",
       "  'duration': 4.38},\n",
       " {'text': 'generates similar attention vectors for',\n",
       "  'start': 615.819,\n",
       "  'duration': 4.591},\n",
       " {'text': 'every English and French word these are',\n",
       "  'start': 618.279,\n",
       "  'duration': 3.691},\n",
       " {'text': 'passed into the feed-forward layer',\n",
       "  'start': 620.41,\n",
       "  'duration': 4.169},\n",
       " {'text': 'linear layer and the softmax layer to',\n",
       "  'start': 621.97,\n",
       "  'duration': 6.63},\n",
       " {'text': \"predict the next word that's the past 2\",\n",
       "  'start': 624.579,\n",
       "  'duration': 7.38},\n",
       " {'text': 'over the architecture explained I hope',\n",
       "  'start': 628.6,\n",
       "  'duration': 4.589},\n",
       " {'text': \"you're understanding more and more\",\n",
       "  'start': 631.959,\n",
       "  'duration': 4.531},\n",
       " {'text': 'details here now for the next pass where',\n",
       "  'start': 633.189,\n",
       "  'duration': 6.931},\n",
       " {'text': 'we go even deeper how exactly do these',\n",
       "  'start': 636.49,\n",
       "  'duration': 7.139},\n",
       " {'text': 'multi-head attention networks look now',\n",
       "  'start': 640.12,\n",
       "  'duration': 5.519},\n",
       " {'text': 'the single headed attention looks like',\n",
       "  'start': 643.629,\n",
       "  'duration': 5.611},\n",
       " {'text': 'this QK and V are abstract vectors that',\n",
       "  'start': 645.639,\n",
       "  'duration': 6.99},\n",
       " {'text': 'extract different components of an input',\n",
       "  'start': 649.24,\n",
       "  'duration': 6.539},\n",
       " {'text': 'word we have QK and V vectors for every',\n",
       "  'start': 652.629,\n",
       "  'duration': 6.45},\n",
       " {'text': 'single word we use these to compute the',\n",
       "  'start': 655.779,\n",
       "  'duration': 5.79},\n",
       " {'text': 'attention vectors for every word using',\n",
       "  'start': 659.079,\n",
       "  'duration': 5.82},\n",
       " {'text': 'this kind of formula for a multi-headed',\n",
       "  'start': 661.569,\n",
       "  'duration': 5.041},\n",
       " {'text': 'attention we have multiple weight',\n",
       "  'start': 664.899,\n",
       "  'duration': 3.031},\n",
       " {'text': 'matrices', 'start': 666.61, 'duration': 4.77},\n",
       " {'text': 'you qwk and WV so we will have multiple',\n",
       "  'start': 667.93,\n",
       "  'duration': 6.469},\n",
       " {'text': 'attention vectors Z for every word',\n",
       "  'start': 671.38,\n",
       "  'duration': 6.66},\n",
       " {'text': 'however our neural net is only expecting',\n",
       "  'start': 674.399,\n",
       "  'duration': 7.361},\n",
       " {'text': 'one attention vector per word so we use',\n",
       "  'start': 678.04,\n",
       "  'duration': 6.93},\n",
       " {'text': 'another weighted matrix wz to make sure',\n",
       "  'start': 681.76,\n",
       "  'duration': 5.22},\n",
       " {'text': 'that the output is still an attention',\n",
       "  'start': 684.97,\n",
       "  'duration': 6.33},\n",
       " {'text': 'vector per word additionally after every',\n",
       "  'start': 686.98,\n",
       "  'duration': 6.359},\n",
       " {'text': 'layer we apply some form of', 'start': 691.3, 'duration': 5.039},\n",
       " {'text': 'normalization typically we would apply a',\n",
       "  'start': 693.339,\n",
       "  'duration': 5.581},\n",
       " {'text': 'patch normalization this movements out',\n",
       "  'start': 696.339,\n",
       "  'duration': 4.531},\n",
       " {'text': 'the law surface making it easier to',\n",
       "  'start': 698.92,\n",
       "  'duration': 4.41},\n",
       " {'text': 'optimize while using larger learning',\n",
       "  'start': 700.87,\n",
       "  'duration': 5.67},\n",
       " {'text': \"rates this is the TLDR but that's what\",\n",
       "  'start': 703.33,\n",
       "  'duration': 5.73},\n",
       " {'text': 'it does but we can actually use',\n",
       "  'start': 706.54,\n",
       "  'duration': 4.01},\n",
       " {'text': 'something called layer normalization',\n",
       "  'start': 709.06,\n",
       "  'duration': 3.93},\n",
       " {'text': 'making the normalization across each',\n",
       "  'start': 710.55,\n",
       "  'duration': 5.56},\n",
       " {'text': \"feature instead of each sample it's\",\n",
       "  'start': 712.99,\n",
       "  'duration': 5.58},\n",
       " {'text': 'better for stabilization if you are',\n",
       "  'start': 716.11,\n",
       "  'duration': 4.38},\n",
       " {'text': 'interested in dabbling in transformer',\n",
       "  'start': 718.57,\n",
       "  'duration': 4.41},\n",
       " {'text': 'code tensorflow has a step-by-step',\n",
       "  'start': 720.49,\n",
       "  'duration': 5.51},\n",
       " {'text': 'tutorial that can get you up to speed',\n",
       "  'start': 722.98,\n",
       "  'duration': 5.07},\n",
       " {'text': 'transformer neural nets have largely',\n",
       "  'start': 726.0,\n",
       "  'duration': 4.54},\n",
       " {'text': 'replaced LS TM nets for sequence to',\n",
       "  'start': 728.05,\n",
       "  'duration': 4.71},\n",
       " {'text': 'vector sequence to sequence and vector',\n",
       "  'start': 730.54,\n",
       "  'duration': 4.47},\n",
       " {'text': 'to sequence problems Google for example',\n",
       "  'start': 732.76,\n",
       "  'duration': 5.31},\n",
       " {'text': 'created Bert which uses transformers to',\n",
       "  'start': 735.01,\n",
       "  'duration': 5.64},\n",
       " {'text': 'pre train models for common NLP tasks',\n",
       "  'start': 738.07,\n",
       "  'duration': 6.06},\n",
       " {'text': \"read that blog it's good however there\",\n",
       "  'start': 740.65,\n",
       "  'duration': 5.46},\n",
       " {'text': 'was another paper called pervasive',\n",
       "  'start': 744.13,\n",
       "  'duration': 4.14},\n",
       " {'text': 'attention that could be even better than',\n",
       "  'start': 746.11,\n",
       "  'duration': 4.41},\n",
       " {'text': 'transformers for sequence to sequence',\n",
       "  'start': 748.27,\n",
       "  'duration': 4.35},\n",
       " {'text': 'models although transformers can be',\n",
       "  'start': 750.52,\n",
       "  'duration': 3.75},\n",
       " {'text': 'better suited for a wider variety of',\n",
       "  'start': 752.62,\n",
       "  'duration': 3.45},\n",
       " {'text': \"problems it's still a very interesting\",\n",
       "  'start': 754.27,\n",
       "  'duration': 3.39},\n",
       " {'text': \"read I'll link it in the description\",\n",
       "  'start': 756.07,\n",
       "  'duration': 4.35},\n",
       " {'text': 'below with other resources so check that',\n",
       "  'start': 757.66,\n",
       "  'duration': 6.39},\n",
       " {'text': 'out hope this helped you get you up to',\n",
       "  'start': 760.42,\n",
       "  'duration': 5.669},\n",
       " {'text': 'speed with transformer neural nets if',\n",
       "  'start': 764.05,\n",
       "  'duration': 3.89},\n",
       " {'text': 'you liked the video hit that like button',\n",
       "  'start': 766.089,\n",
       "  'duration': 5.071},\n",
       " {'text': 'subscribe to stay up to date with some',\n",
       "  'start': 767.94,\n",
       "  'duration': 4.51},\n",
       " {'text': 'deep learning and machine learning',\n",
       "  'start': 771.16,\n",
       "  'duration': 4.169},\n",
       " {'text': 'knowledge and I will see you guys in the',\n",
       "  'start': 772.45,\n",
       "  'duration': 6.02},\n",
       " {'text': 'next one bye bye', 'start': 775.329, 'duration': 3.141}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bcfed2a0-b98d-4001-8008-dcafd3e07be3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recurrent neural Nets they are \"time:00:00:00\" feed-forward neural networks rolled out \"time:00:00:02\" over time as such they deal with \"time:00:00:04\" sequence data where the input has some \"time:00:00:07\" defined ordering this gives rise to \"time:00:00:09\" several types of architectures the first \"time:00:00:12\" is vector to sequence models these \"time:00:00:15\" neural nets \"time:00:00:18\" take in a fixed size vector as input and \"time:00:00:18\" it outputs a sequence of any length in \"time:00:00:21\" image captioning for example the input \"time:00:00:24\" can be a vector representation of an \"time:00:00:27\" image and the output sequence is a \"time:00:00:28\" sentence that describes the image the \"time:00:00:31\" second type is a sequence to vector \"time:00:00:34\" model these neural networks taken a \"time:00:00:36\" sequences input and spits out a fixed \"time:00:00:39\" length vector in sentiment analysis the \"time:00:00:40\" movie review is an input and a fixed \"time:00:00:44\" size vector is the output indicating how \"time:00:00:46\" good or bad this person thought the \"time:00:00:48\" movie was sequence to sequence models is \"time:00:00:51\" the more popular variant and these \"time:00:00:55\" neural networks taken a sequences input \"time:00:00:57\" and outputs another sequence so for \"time:00:00:59\" example language translation the input \"time:00:01:02\" could be a sentence in Spanish and the \"time:00:01:05\" output is the translation in English do \"time:00:01:07\" you have some time series data to model \"time:00:01:12\" well our nen\\'s would be the go-to \"time:00:01:13\" however rnns have some problems our nuns \"time:00:01:15\" are slow so slow that we use a truncated \"time:00:01:19\" version of back propagation to Train it \"time:00:01:23\" and even that\\'s too Hardware intense and \"time:00:01:25\" also they can\\'t deal with long sequences \"time:00:01:28\" very well we get gradients that vanish \"time:00:01:31\" and explode if the network is too long \"time:00:01:33\" in comes lsdm networks in 1991 that \"time:00:01:36\" introduced a long short term memory cell \"time:00:01:39\" in place of dumb neurons this cell has a \"time:00:01:42\" branch that allows passed information to \"time:00:01:46\" skip a lot of the processing of the \"time:00:01:48\" current cell and move on to the next \"time:00:01:50\" this allows the memory to be retained \"time:00:01:52\" for longer sequences now to that second \"time:00:01:55\" point we seem to be able to deal with \"time:00:01:58\" longer sequences well or are we well \"time:00:02:00\" kind of probably if the order of \"time:00:02:04\" hundreds of words instead of a thousand \"time:00:02:07\" words however to the first point normal \"time:00:02:09\" our ends are \"time:00:02:12\" but LS TMS are even slower they are more \"time:00:02:14\" complex for these RN and LST M networks \"time:00:02:17\" input data needs to be passed \"time:00:02:21\" sequentially or serially one after the \"time:00:02:23\" other \"time:00:02:25\" we need inputs of the previous state to \"time:00:02:26\" make any operations on the current state \"time:00:02:28\" such sequential flow does not make use \"time:00:02:31\" of today\\'s GPUs very well which are \"time:00:02:34\" designed for parallel computation so \"time:00:02:36\" question how can we use parallelization \"time:00:02:40\" for sequential data in 2017 the \"time:00:02:42\" Transformer neural network architecture \"time:00:02:46\" was introduced the network employs an \"time:00:02:48\" encoder decoder architecture much like \"time:00:02:51\" recurrent neural Nets the difference is \"time:00:02:53\" that the input sequence can be passed in \"time:00:02:57\" parallel consider translating a sentence \"time:00:02:59\" from English to French I\\'ll use this as \"time:00:03:02\" a running example throughout the video \"time:00:03:04\" with an RNN encoder we pass an input \"time:00:03:06\" English sentence one word after the \"time:00:03:09\" other the current words hidden state has \"time:00:03:11\" dependencies in the previous words \"time:00:03:14\" hidden state the word embeddings are \"time:00:03:16\" generated one time step at a time with a \"time:00:03:18\" transformer encoder on the other hand \"time:00:03:22\" there is no concept of time step for the \"time:00:03:24\" input we pass in all the words of the \"time:00:03:26\" sentence simultaneously and determine \"time:00:03:29\" the word embeddings simultaneously so \"time:00:03:30\" how is it doing this let\\'s pick a part \"time:00:03:33\" the transformer architecture I\\'ll make \"time:00:03:36\" multiple passes on the explanation in \"time:00:03:38\" the first pass will be like a high \"time:00:03:40\" overview and the next rounds we\\'ll get \"time:00:03:42\" into more details let\\'s start with input \"time:00:03:43\" embeddings computers don\\'t understand \"time:00:03:47\" words they get numbers they get vectors \"time:00:03:49\" and matrices \"time:00:03:52\" the idea is to map every word to a point \"time:00:03:53\" in space where similar words in meaning \"time:00:03:55\" are physically closer to each other the \"time:00:03:57\" space in which they are present is \"time:00:03:59\" called an embedding space we could pre \"time:00:04:01\" train this embedding space to save time \"time:00:04:03\" or even just use an already pre trained \"time:00:04:05\" embedding space this embedding space \"time:00:04:07\" Maps a word to a vector but the same \"time:00:04:10\" word in different sentences may have \"time:00:04:13\" different meanings this is where \"time:00:04:15\" positional encoders come in it\\'s a \"time:00:04:17\" vector that has information on distances \"time:00:04:20\" between words and the sentence the \"time:00:04:22\" original paper uses a sine and cosine \"time:00:04:25\" function to generate this vector but it \"time:00:04:27\" could be any reasonable function after \"time:00:04:30\" passing the English sentence through the \"time:00:04:32\" input embedding and applying the \"time:00:04:34\" positional encoding we get word vectors \"time:00:04:37\" that have positional information that is \"time:00:04:39\" context nice we pass this in to the \"time:00:04:42\" encoder block where it goes through a \"time:00:04:47\" multi-headed attention layer and a \"time:00:04:49\" feed-forward layer okay one at a time \"time:00:04:51\" attention it involves answering what \"time:00:04:54\" part of the input should I focus on if \"time:00:04:58\" we are translating from English to \"time:00:05:00\" French and we are doing self attention \"time:00:05:02\" that is attention with respect to \"time:00:05:05\" oneself the question we want to answer \"time:00:05:07\" is how relevant is the ithe word in the \"time:00:05:10\" English sentence relevant to other words \"time:00:05:14\" in the same English sentence this is \"time:00:05:16\" represented in the I thought ention \"time:00:05:20\" vector and it is computed in the \"time:00:05:22\" attention block for every word we can \"time:00:05:24\" have an attention vector generated which \"time:00:05:28\" captures contextual relationships \"time:00:05:30\" between words in the sentence so that\\'s \"time:00:05:32\" great \"time:00:05:35\" the other important unit is a \"time:00:05:35\" feed-forward net this is just a simple \"time:00:05:38\" feed-forward neural network that is \"time:00:05:41\" applied to every one of the attention \"time:00:05:43\" vectors these feed-forward nets are used \"time:00:05:44\" in practice to transform the attention \"time:00:05:47\" vectors into a form that is digestible \"time:00:05:49\" by the next encoder block or decoder \"time:00:05:52\" block now that\\'s the high-level overview \"time:00:05:54\" of the encoder components let\\'s talk \"time:00:05:57\" about the decoder now during the \"time:00:06:00\" training phase for English to French we \"time:00:06:03\" feed the output French sentence to the \"time:00:06:05\" decoder but remember computers don\\'t get \"time:00:06:08\" language they get numbers vectors and \"time:00:06:11\" matrices so we process it using the \"time:00:06:13\" input embedding to get the vector form \"time:00:06:16\" of the word and then we add a positional \"time:00:06:18\" vector to get the notion of context of \"time:00:06:21\" the word in a sentence we pass this \"time:00:06:23\" vector finally into a decoder block that \"time:00:06:26\" has three main components two of which \"time:00:06:30\" are similar to the encoder block the \"time:00:06:32\" self attention block generates attention \"time:00:06:35\" vectors for every word in the french \"time:00:06:37\" sentence to represent how much \"time:00:06:39\" each word is related to every word in \"time:00:06:41\" the same sentence these attention \"time:00:06:43\" vectors and vectors from the encoder are \"time:00:06:46\" passed into another attention block \"time:00:06:48\" let\\'s call this the encoder decoder \"time:00:06:50\" attention block since we have one vector \"time:00:06:53\" from every word in the English and \"time:00:06:55\" French sentences this attention block \"time:00:06:58\" will determine how related each word \"time:00:07:02\" vector is with respect to each other and \"time:00:07:04\" this is where the main English to French \"time:00:07:07\" word mapping happens the output of this \"time:00:07:09\" block is attention vectors for every \"time:00:07:12\" word in English and the French sentence \"time:00:07:14\" each vector representing the \"time:00:07:17\" relationships with other words in both \"time:00:07:19\" the languages next we pass each \"time:00:07:21\" attention vector to a feed-forward unit \"time:00:07:25\" this makes the output vector more \"time:00:07:28\" digestible by the next decoder block or \"time:00:07:30\" a linear layer now the linear layer is \"time:00:07:32\" surprise-surprise \"time:00:07:36\" another feed for connected layer it\\'s \"time:00:07:37\" used to expand the dimensions into the \"time:00:07:40\" number of words in the french language \"time:00:07:42\" the softmax layer transforms it into a \"time:00:07:44\" probability distribution which is now \"time:00:07:46\" human interpretable and the final word \"time:00:07:49\" is the word corresponding to the highest \"time:00:07:51\" probability overall this decoder \"time:00:07:53\" predicts the next word and we execute \"time:00:07:56\" this over multiple time steps until the \"time:00:07:58\" end of sentence token is generated \"time:00:08:01\" that\\'s our first passed over the \"time:00:08:03\" explanation of the entire network \"time:00:08:06\" architecture for transformers but let\\'s \"time:00:08:08\" go over it again but this time introduce \"time:00:08:11\" even more details going deeper an input \"time:00:08:12\" English sentence is converted into an \"time:00:08:17\" embedding to represent meaning we add a \"time:00:08:19\" positional vector to get the context of \"time:00:08:22\" the word in the sentence our attention \"time:00:08:24\" block computes the attention vectors for \"time:00:08:27\" each word only problem here is that the \"time:00:08:29\" attention vector may not be too strong \"time:00:08:33\" for every word the attention vector may \"time:00:08:35\" weight its relation with itself much \"time:00:08:38\" higher it\\'s true but it\\'s useless we are \"time:00:08:40\" more interested in interactions with \"time:00:08:43\" different words and so we determine like \"time:00:08:44\" eight such attention vectors per word \"time:00:08:47\" and take a weighted average to compute \"time:00:08:49\" the final attention vector for every \"time:00:08:52\" word \"time:00:08:54\" since we use multiple attention vectors \"time:00:08:55\" we call it the multi-head attention \"time:00:08:57\" block the attention vectors are passed \"time:00:08:59\" in through a feed-forward net one vector \"time:00:09:02\" at a time the cool thing is that each of \"time:00:09:04\" the attention nets are independent of \"time:00:09:07\" each other so we can use some beautiful \"time:00:09:09\" parallelization here because of this we \"time:00:09:11\" can pass all our words at the same time \"time:00:09:14\" into the encoder block and the output \"time:00:09:16\" will be a set of encoded vectors for \"time:00:09:18\" every word \"time:00:09:20\" now the decoder we first obtained the \"time:00:09:21\" embedding of French words to encode \"time:00:09:24\" meaning then add the positional value to \"time:00:09:27\" retain context they are then passed to \"time:00:09:30\" the first attention block the paper \"time:00:09:34\" calls this the masked attention block \"time:00:09:36\" why is this the case though it\\'s because \"time:00:09:38\" while generating the next French word we \"time:00:09:41\" can use all the words from the English \"time:00:09:44\" sentence but only the previous words of \"time:00:09:45\" the French sentence if we are going to \"time:00:09:48\" use all the words in the French sentence \"time:00:09:51\" then there would be no learning it would \"time:00:09:52\" just spit out the next word so while \"time:00:09:54\" performing parallelization with matrix \"time:00:09:58\" operations we make sure that the matrix \"time:00:09:59\" will mask the words appearing later by \"time:00:10:02\" transforming it into zeros so the \"time:00:10:05\" attention network can\\'t use them the \"time:00:10:07\" next detention block which is the \"time:00:10:12\" encoder decoder attention block \"time:00:10:14\" generates similar attention vectors for \"time:00:10:16\" every English and French word these are \"time:00:10:18\" passed into the feed-forward layer \"time:00:10:20\" linear layer and the softmax layer to \"time:00:10:22\" predict the next word that\\'s the past 2 \"time:00:10:25\" over the architecture explained I hope \"time:00:10:29\" you\\'re understanding more and more \"time:00:10:32\" details here now for the next pass where \"time:00:10:33\" we go even deeper how exactly do these \"time:00:10:36\" multi-head attention networks look now \"time:00:10:40\" the single headed attention looks like \"time:00:10:44\" this QK and V are abstract vectors that \"time:00:10:46\" extract different components of an input \"time:00:10:49\" word we have QK and V vectors for every \"time:00:10:53\" single word we use these to compute the \"time:00:10:56\" attention vectors for every word using \"time:00:10:59\" this kind of formula for a multi-headed \"time:00:11:02\" attention we have multiple weight \"time:00:11:05\" matrices \"time:00:11:07\" you qwk and WV so we will have multiple \"time:00:11:08\" attention vectors Z for every word \"time:00:11:11\" however our neural net is only expecting \"time:00:11:14\" one attention vector per word so we use \"time:00:11:18\" another weighted matrix wz to make sure \"time:00:11:22\" that the output is still an attention \"time:00:11:25\" vector per word additionally after every \"time:00:11:27\" layer we apply some form of \"time:00:11:31\" normalization typically we would apply a \"time:00:11:33\" patch normalization this movements out \"time:00:11:36\" the law surface making it easier to \"time:00:11:39\" optimize while using larger learning \"time:00:11:41\" rates this is the TLDR but that\\'s what \"time:00:11:43\" it does but we can actually use \"time:00:11:47\" something called layer normalization \"time:00:11:49\" making the normalization across each \"time:00:11:51\" feature instead of each sample it\\'s \"time:00:11:53\" better for stabilization if you are \"time:00:11:56\" interested in dabbling in transformer \"time:00:11:59\" code tensorflow has a step-by-step \"time:00:12:00\" tutorial that can get you up to speed \"time:00:12:03\" transformer neural nets have largely \"time:00:12:06\" replaced LS TM nets for sequence to \"time:00:12:08\" vector sequence to sequence and vector \"time:00:12:11\" to sequence problems Google for example \"time:00:12:13\" created Bert which uses transformers to \"time:00:12:15\" pre train models for common NLP tasks \"time:00:12:18\" read that blog it\\'s good however there \"time:00:12:21\" was another paper called pervasive \"time:00:12:24\" attention that could be even better than \"time:00:12:26\" transformers for sequence to sequence \"time:00:12:28\" models although transformers can be \"time:00:12:31\" better suited for a wider variety of \"time:00:12:33\" problems it\\'s still a very interesting \"time:00:12:34\" read I\\'ll link it in the description \"time:00:12:36\" below with other resources so check that \"time:00:12:38\" out hope this helped you get you up to \"time:00:12:40\" speed with transformer neural nets if \"time:00:12:44\" you liked the video hit that like button \"time:00:12:46\" subscribe to stay up to date with some \"time:00:12:48\" deep learning and machine learning \"time:00:12:51\" knowledge and I will see you guys in the \"time:00:12:52\" next one bye bye \"time:00:12:55\" '"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396c2c1-37d7-4f2f-9a87-7eb800796dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c355515-8f4c-4a79-9964-e3dfab1801d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b99651-aa34-4c75-9e2d-c1a7a4731bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a10308-50ad-4d69-a442-6d5f7df0b6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dcc048bd-3ac0-43ba-9e62-d41120e870d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-hub-youtube-transcript\n",
      "  Downloading llama-hub-youtube-transcript-0.0.1.tar.gz (3.4 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting requests-html (from llama-hub-youtube-transcript)\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from llama-hub-youtube-transcript) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->llama-hub-youtube-transcript) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->llama-hub-youtube-transcript) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->llama-hub-youtube-transcript) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->llama-hub-youtube-transcript) (2024.8.30)\n",
      "Collecting pyquery (from requests-html->llama-hub-youtube-transcript)\n",
      "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting fake-useragent (from requests-html->llama-hub-youtube-transcript)\n",
      "  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting parse (from requests-html->llama-hub-youtube-transcript)\n",
      "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting bs4 (from requests-html->llama-hub-youtube-transcript)\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting w3lib (from requests-html->llama-hub-youtube-transcript)\n",
      "  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pyppeteer>=0.0.14 (from requests-html->llama-hub-youtube-transcript)\n",
      "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html->llama-hub-youtube-transcript)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /opt/conda/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html->llama-hub-youtube-transcript) (8.4.0)\n",
      "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html->llama-hub-youtube-transcript)\n",
      "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html->llama-hub-youtube-transcript) (4.66.5)\n",
      "Collecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html->llama-hub-youtube-transcript)\n",
      "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from bs4->requests-html->llama-hub-youtube-transcript) (4.12.3)\n",
      "Collecting lxml>=2.1 (from pyquery->requests-html->llama-hub-youtube-transcript)\n",
      "  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting cssselect>=1.2.0 (from pyquery->requests-html->llama-hub-youtube-transcript)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html->llama-hub-youtube-transcript) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html->llama-hub-youtube-transcript) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->bs4->requests-html->llama-hub-youtube-transcript) (2.6)\n",
      "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
      "Downloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
      "Downloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
      "Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "Building wheels for collected packages: llama-hub-youtube-transcript\n",
      "  Building wheel for llama-hub-youtube-transcript (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-hub-youtube-transcript: filename=llama_hub_youtube_transcript-0.0.1-py3-none-any.whl size=2949 sha256=1308df1e56944daa0f63f29d0cd9375c81dc41d0fdd8565250fe6e2baede3207\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/f2/62/8b/d100fcb7945ed8210b123930f9a12514c9d6203c9bf9f850bf\n",
      "Successfully built llama-hub-youtube-transcript\n",
      "Installing collected packages: parse, fake-useragent, appdirs, websockets, w3lib, pyee, lxml, cssselect, pyquery, pyppeteer, bs4, requests-html, llama-hub-youtube-transcript\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 13.1\n",
      "    Uninstalling websockets-13.1:\n",
      "      Successfully uninstalled websockets-13.1\n",
      "Successfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.2.0 fake-useragent-1.5.1 llama-hub-youtube-transcript-0.0.1 lxml-5.3.0 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 w3lib-2.2.1 websockets-10.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting llama-index-readers-youtube-transcript\n",
      "  Downloading llama_index_readers_youtube_transcript-0.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-youtube-transcript) (0.11.19)\n",
      "Collecting youtube-transcript-api>=0.5.0 (from llama-index-readers-youtube-transcript)\n",
      "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (2024.9.0)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (3.4)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.14.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (4.0.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (3.23.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (24.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-youtube-transcript) (1.2.2)\n",
      "Downloading llama_index_readers_youtube_transcript-0.2.0-py3-none-any.whl (3.6 kB)\n",
      "Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: youtube-transcript-api, llama-index-readers-youtube-transcript\n",
      "Successfully installed llama-index-readers-youtube-transcript-0.2.0 youtube-transcript-api-0.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-hub-youtube-transcript\n",
    "\n",
    "%pip install llama-index-readers-youtube-transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5d3a7-7bf4-4eb9-bd9f-42adf679ea73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c076fda3-f476-4860-a4e3-b5844fd39c39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.readers.youtube_transcript import YoutubeTranscriptReader\n",
    "\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(\n",
    "    ytlinks=[\"https://www.youtube.com/watch?v=i3OYlaoj-BM\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff821cca-b722-4d89-b62e-fa62d5844659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bed9e70a-9115-43b7-b321-ab02912bb13e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id_</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'i3OYlaoj-BM'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'video_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'i3OYlaoj-BM'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">excluded_embed_metadata_keys</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">excluded_llm_metadata_keys</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">relationships</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"the imaginative laws scale visual\\nrecognition challenge was a\\nworld-changing competition\\nthat ran from</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">around 2010 to 2017.\\nduring his time the competition acted as\\nthe place to go if you needed to find\\nwhat the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">current state of the art was in\\nimage classification object localization\\nobject detection as well as that\\n2012 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">onwards it really acted as the\\nCatalyst of the explosion in deep\\nlearning researchers fine-tuned </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">better\\nperforming computer vision models year\\non year but there was a unquestioned\\nassumption causing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">problems\\nit was assumed that every new task\\nrequired model fine tuning this required\\na lot of data and a lot of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data required\\na lot of capital and time it wasn't\\nuntil recently that this assumption was\\nchallenged and proven </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">wrong the\\nastonishing rise of what are called\\nmulti-modal models has made the what was\\nthought impossible very </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">possible across\\nvarious domains and tasks one of those\\nis called zero shot object detection and\\nlocalization now</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">zero shot refers to\\ntaking a model and applying it to a new\\ndomain without ever fine-tuning it on\\ndata from that</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">new domain so that means\\nwe can take a model who can it maybe it\\nworks in in one domain classification in\\none </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">particular area on one data set and\\nwe can take that same model without any\\nfine tuning and we can use it for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">object\\ndetection in a completely different\\ndomain without that model seeing any\\ntraining data from that new </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">domain so in\\nthis video we're going to explore how to\\nuse open ai's clip for zero shots object\\ndetection and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">localization let's begin\\nwith taking a quick look at image\\nclassification now image classification\\ncan kind of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be seen as one of the\\nsimplest tasks in visual recognition and\\nit's also the first step on the way to\\nobject </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">detection at his core it's just\\nassigning a categorical label to an\\nimage now moving on from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">image\\nclassification we have object\\nlocalization object localization is\\nimage classification followed by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the\\nidentification of where in the image the\\nspecific object actually is so we are\\nlocalizing the the object now</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">doing that\\nwhere essentially just going to identify\\nthe coordinates on the image I going to\\nreturn the typical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approach to this is\\nreturn an image where you have like a\\nbounding box surrounding the the object\\nthat you are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">looking for and then we\\ntake this one step further to perform\\nobject detection with detection we are\\nlocalizing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multiple objects within the\\nimage or we have the capability to\\nidentify multiple objects within the\\nimage so in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this example we have cat and\\na dog we would expect with object\\ndetection to identify both the cat and\\nthe dot in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the case of us having\\nmultiple dolbs in this image almost Cuts\\nin this image we would also expect the\\nobject </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">detection algorithm to actually\\nidentify each one of those independently\\nnow in the past if we wanted to switch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a\\nmodel between anyone these tasks would\\nhave to fine-tune or more data you want\\nto switch it to another domain </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we would\\nhave to also fine tune it on new data\\nfrom that domain but that's not always\\nthe case with models like </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">open ai's clip\\nfor performing each one of these tasks\\nin a zero shot setting now open ai's\\nclip is a multi-modal</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model that has\\nbeen pre-trained on a huge number of\\ntext and image Pairs and it essentially\\nworks by identifying</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">text and image\\npairs that have a similar meaning and\\nplacing them within a similar Vector\\nspace so every text </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and every image gets\\nconverted into a vector and they are\\nplaced in a shared Vector space and the\\nvectors that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">appear close together they\\nhave a similar meaning now Clips very\\nbroad pre-training means that it can\\nperform </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">very effectively across a lot of\\ndifferent domains it's seen a lot of\\ndata and so it has a good understanding\\nof</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">all these different things and we can\\neven adjust the task being performed\\nwith just a few code changes we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">don't\\nactually have to is the model itself we\\njust adjust the code around the model\\nand that's very much thanks </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to Clips\\nfocus on sort of comparing these vectors\\nso for example for classification we\\ngive clip a list of our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">plus labels and\\nthen we pass in the images and we just\\nidentify within that space where those\\nimages are with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respect to those plus\\nlabel vectors and which plus label is\\nthe most similar to our particular image\\nand then </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that is our prediction so that\\nmost similar plus label\\nthat's our predated class now for\\nobjects localization we</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">apply a very\\nsimilar type of logic as before we\\ncreate a class label but unlike before\\nwe don't need the entire </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">image into clip\\nto localize an object we have to break\\nthe image into patches we then pass a\\nwindow over all of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">those patches moving\\nacross the entire image left to right\\ntop to bottom and we generate a image\\nembedding for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">each of those windows and\\nthen we calculate the similarity between\\neach one of those windows embedded by\\nclip </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and the class label embedding\\nreturning a similarity score for every\\nsingle patch now after calculating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the\\nsimilarity score for every single patch\\nwe use that to create almost like a map\\nof relevance across the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entire image and\\nthen we can use that map to identify the\\nlocation of the object of interest and\\nfrom that we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will get something that's\\nkind of like this so we have most of the\\nimage will be very dark and black that\\nmeans </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as the object of interest is not\\nin that space and then using that\\nlocalization map we can create a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more\\ntraditional bounding box visualization\\nas well both of these visuals are\\ncapturing the same information </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we're\\njust displaying it in a different way\\nnow there's also other approaches to\\nthis so I recently hosted a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">talk with\\nwhat two two sets of people actually so\\nFederico Bianchi from Stanford's NLP\\ngroup and also Raphael </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pissoni and both\\nof those have worked on a Italian clip\\nproject and part of that was performing\\nobject </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">localization now to do that they\\nuse a slightly different approach what\\nI'm going to demonstrate here and we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can\\nthink of it as almost like the opposite\\nso whereas we slide a window over the\\nwhole image they slide a black</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">patch\\nover the whole image which hides what is\\nbehind in that patch and then they feed\\nthe image into click and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">essentially as\\nyou slide the patch over the image you\\nare hiding a part of the image and\\ntherefore if this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">similarity score drops\\nwhen the patch is over a certain area\\nyou know that the object you're looking\\nfor is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">probably within that space and\\nthat's called the occlusion algorithm\\nnow moving on to object detection which\\nis </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">like the last level in these three\\ntasks we will be identifying multiple\\nobjects now there's a very fine </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">line\\nbetween object localization and object\\ndetection but you can simply think of it\\nas localization for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multiple clusters\\nand multiple objects with our cat and\\nButterfly image we will be searching for\\ntwo objects a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cat and a butterfly and\\nwith that we could draw a bounding box\\naround both of those objects and\\nessentially what</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we're doing now is\\nusing localization for a single object\\nbut then we're putting both of those\\ntogether in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">loop in our code and we're\\nproducing this object detection process\\nnow we've covered the idea behind my\\nimage </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">classification onto object\\nlocalization and object detection now\\nlet's have a look at how we actually\\nImplement </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">all of this now before we move\\non to any classification localization or\\ndetection task we need to have some </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data\\nwe're going to use a small demo data set\\ncalled James Callum image text demo and\\nwe can download it like </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this so we're\\nusing hooking phase data sets here which\\nwe can pip install with Pip install\\ndata sets\\nand this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is the day so it's very small\\nit's 21 text to image pairs okay one of\\nthose is the image you've already seen\\nthe</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cat with a butterfly landing on its\\nnose very curious how they got that\\nphoto now after you've downloaded </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that\\ndata set\\nwe're going to be using this image here\\nand what we want to do is not use the\\nimage file itself </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">because at the moment\\nit's a it's a pill python image object\\nbut instead we need to convert it into a\\ncanister </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">now we're going to be using pi\\ntorch later on so what I want to do here\\nis we're going to just transform </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the\\nimage into a tensor and we use toxin\\ntransforms use the typical pipeline tool\\nin computer vision and we just</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">use tube\\ntensor okay and then we process our\\nimage through that Pipeline and then we\\ncan see that we get this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">okay so what\\nare these values here we have the height\\nof the image in pixels\\nthe width of the image in pixels </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nthen also the three color channels red\\ngreen and blue that make up the image\\nnow we need a slightly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">different format\\nwhen we are processing everything one we\\nneed to add those patches and two we\\nneed to process </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">it through a pie torch\\nmodel and we also need the batch\\ndimension for that so the first thing\\nwe're going to do </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is add the batch\\nDimension it's just a single image so we\\njust have one in there but we we need\\nthat anyway and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">then we come down to\\nhere so this is where we're going to\\nbreak the image into the patches okay\\neach patch is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">going to be 256 dimensions\\nin both height and width so the first\\nthing we do here is unfold and we get\\nthis here</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">okay there's two five six and\\nthere's 20. now the 20 is the height of\\nthe image in these 256 pixel patches\\nand </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we can visualize that here\\nall right so now we have all these kind\\nof like slivers of the image that's just\\na </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vertical component of each patch\\nand we use unfold again but in this time\\nin the second Dimensions the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">targeting\\nwhat was this Dimension here and we also\\nget another 256 now we visualize that we\\nget our four </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">patches\\nokay like this\\nnow if you just consider this here it's\\nlike if we look at this patch here it\\ndoesn't </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tell us anything about the image\\nright and even when we're over cats\\nthese patches are way too small to\\nactually</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tell us anything if clip is\\nprocessing a single patch at a time\\nit's probably not going to tell us\\nanything </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">maybe it could tell us that\\nthere's some hair in this patch or that\\nthere's an eye in this patch but beyond\\nthat</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">it's not going to be very useful so\\nrather than feeding single patches into\\nclip what we do is actually feed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a\\nwindow a six by six patches or we can\\nmodify that value if we prefer and that\\njust gives us a big patch to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pass over\\nto clip now the reason that we don't\\njust do that from the start we don't\\njust create these bigger </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">patches to\\nbegin with is because when we're sliding\\nthrough the image we want to have some\\ndegree of overlap </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between each patch\\nokay so we create these smaller patches\\nand then what we can do is actually\\nslide across just</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">one little patch at a\\ntime and we Define that using the stride\\nvariable so if we come down to here\\nwe have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">window we have stride\\nremove this\\nand here we go this is our code for\\ngoing through the whole image creating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a\\npatch at every time step okay so we go\\nfor y and we go through the whole y-axis\\nand then within that we're </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">going across\\nleft to right with each step and we\\ninitialize a empty big patch array so\\nthis is our like the full</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">window\\nwe got the current batch so okay let's\\nsay we start at zero zero X zero y zero\\nwe go from zero to six and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">zero to six\\nhere\\nright so that gives us the very top left\\ncorner or window of the image and then\\nwe're </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">literally going through and and\\njust go processing all of that and you\\ncan see that happening here as wine </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">eggs\\nare increasing we're moving through that\\nimage and we're seeing each big patch\\nfrom our image okay sliding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across with\\na single small little patch at a time so\\nthat we don't miss any important\\ninformation\\nnow this is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">how we're going to run\\nthrough the whole image but before we do\\nthat we actually need clip so let's go\\nahead and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">actually initialize clip\\nso to do that all we do is this so we're\\nusing hook and face Transformers which\\nis </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using pi torch in the in the back\\nthere so we need the clip processor\\nwhich is like a pre-processing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pipeline\\nfor both text and images and then the\\nactual model itself okay so we some\\nmodel ID and we initialize </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">both of those\\nthen what we want to do is move the\\nmodel to advice if possible all right so\\nwe can use CPU but if</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you have a Kudo\\nenabled GPU that will be here much\\nfaster so I'd recommend doing that if\\nyou can if not then you</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can use CPU it\\nwill be a bit slower but it will still\\nrun within a variable time frame so if\\nI'm running this on</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">my Mac I am using a\\nCPU you can actually run this on MPS as\\nwell so you could change your device to\\nMPS if you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">have an MPS enabled Apple\\nsilicon device\\nso now returning to that process where\\nwe're going through each window </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">within\\nthe image we're just going to add a\\nlittle bit more logic so we are\\nprocessing like we were before </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there's\\nnothing different here we're creating\\nthat big patch and then what we do is\\nprocess that big patch and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">process a\\ntext label okay so at the moment we're\\nlooking for a fluffy cat within this\\nimage so that is how we do</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this we're\\nreturning Pi torch tensors we also add\\npadding here as well for the text\\nalthough in this case I </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">don't think we\\nneed it because we only have a single\\ntext item but we include that when we're\\nusing multiple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">text items later and then\\nwe calculate and retrieve the similarity\\nscore between them okay so if we pass\\nboth </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">text and images through this\\nprocessor we'll pass both into our\\ninputs here and then we just calculate\\nthe or we</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extract the logits for each\\nimage and the item just converts that\\ninto a array of values or single value\\nand </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">then here we have those scores so\\nwhat we're doing here is creating the\\nwhat I earlier called like the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevance\\nmap or localization map throughout the\\nwhole image so for every window that we\\ngo through we're adding</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this score to\\nevery single patch or little patch\\nwithin that window and what we're going\\nto do or what we're </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">going to find when\\nwe do that is that some patches will\\nnaturally have a high score than others\\nbecause they are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">viewed more times right\\nso if you think about the top left patch\\nin the image that's only going to be\\nviewed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">once whereas patches in the\\nmiddle are going to be viewed many times\\nbecause we'll have a sliding window\\ngoing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">over there multiple times so what\\nwe also need to do is identify the\\nnumber of runs that we perform or number\\nof</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">calculations that we perform within\\neach one of those patches the reason we\\ndo that is so that we can take </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the\\naverage for each score based on the\\nnumber of times that score has been\\ncapital related because here </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we're\\ntaking the total Awards scores and then\\nwe'll just take the average like so now\\nthe scores tensor is going</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to have a\\nvery smooth gradient of values from zero\\ncompletely irrelevant to one now if you\\nconsider that we've </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">been going over\\nthese scores multiple times it means\\nthat the object of interest is kind of\\nlike faded out of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the window like over\\nmultiple steps so that means that the\\nsimilarities score quite gradually faves\\nout as you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">go away from the object which\\nmeans that you don't really get very\\ngood localization if you use these\\nscores </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">directly so what you what we need\\nto do is actually clip the lower scores\\ndown to zero so to do that what it </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is\\ncalculate the average of scores across\\nthe whole image we subtract that average\\nfrom the current scores what </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that will\\ndo is push 50 of the scores below zero\\nand then we click those scores so\\nanything below zero becomes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">zero and we\\ncan do this multiple times okay one time\\nis usually enough but you can do it\\nmultiple times to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase that effect\\nof making the edge of this detected or\\nlocalized area better defined and then\\nafter you've </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">done that what we need to\\ndo is normalize those scores okay so we\\nmight have to do this a few times </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">but\\neverything's probably going to be within\\nthe range of like zero to 0.5 or 0 to\\n0.2 so then we normalize </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">those scores\\nbring them back within the range of zero\\nto one now to apply these scores to the\\npatches we need to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">align their tenses\\nbecause right now that they are not\\naligned okay for the scores we just we\\nhave like 20 by 13</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tensor but for the\\npatches we have the the batch Dimension\\nthere we have the 20 by 13 which we do\\nwant but then </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we have the three color\\nchannels and the two five six for each\\num set of pixels within each patch so we\\nneed to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">adjust that a little bit so we\\nneed to First remove the batch Dimension\\nwe do that by squeezing out the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">zero\\nDimension which is a batch Dimension and\\nthen we permute the different dimensions\\nit's actually just moving</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">them around in\\nour patches in order to align them\\nbetter with the score tensor dimensions\\nand then all we do is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multiply the\\npatches by those scores that's pretty\\nstraightforward\\nthen we have to mute them again because\\nif </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we want to visualize everything needs\\nto be within a certain shape in order\\nfor us to visualize our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">matplotlib\\nso we come down and the first thing you\\ndo is just get y next here so Y and X\\nare the the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">patches\\nsee here this is y so the height of the\\nimage in patches and then 13 which is\\nthe width of the image in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">patches and we\\ncome down here and we can plot this okay\\nand we get this it's pretty nice visual\\nlocalizes the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The Fluffy Cuts within\\nthat image now what's really interesting\\nis if we just search for a cat we\\nactually get a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">slightly different\\nlocalization because here you can see\\nit's kind of focusing a lot on the\\nfluffy part of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cat so if we just\\nsearch for a cat it would actually focus\\nmore on the head so we can really add\\nnuanced </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information to these prompts and\\nget a pretty nuanced response back\\nnow we can do the same for butterfly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">so\\nwe'll just throw all that code together\\nthis is just what we've done before we\\ninitialize scores and runs and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we go\\nprocess all of that the only thing we\\nchange here is the prompt we change it\\nto a butterfly and if we go </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">down and\\nwe're going to go down and down and\\nvisualize that we'll get this okay so\\nagain that's that's pretty </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cool we can\\nsee that it is identifying where in the\\nimage that butterfly actually is so that\\nis the object </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">localization set\\nnow I want to have a look at object\\ndetection which is essentially just\\ntaking the object </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">localization and and\\nwrapping some more code around it\\nin order to look at these multiple\\nobjects rather than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just one but to do\\nthat we can't really visualize in the\\nsame way that we've done here we're\\ngoing to need a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">different type of\\nvisualization and that's where we have\\nthe bounding boxes so let's take a look\\nat how we would</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">do that so using the I\\nthink the butterfly examples of\\nbutterfly scores that we just calculated\\nwe're going to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">look at where those\\nscores are higher than 0.5 now you can\\nadjust this threshold based on you know\\nwhat you find</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">works best so we do this\\nand what we'll get is a array of true\\nand false values as to where the score\\nwas higher</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">than 0.5 and not\\nand then we detect where the non-zero\\nvalues are in that array and what we do\\nis get a load of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">X and Y values here\\nso position three two we know that there\\nis a score that is higher than 0.5 and\\nwe get three</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and two here so three is\\nthe row of the non-zero value and 2 is a\\ncolumn of the non-zero value so at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">row\\nposition three and column two we know\\nthat there is a non-zero value or a\\nvalue or score that's higher than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">0.5\\nour threshold\\nand put all that together we'll get\\nsomething looks kind of like this so we\\nalready we kind </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of see that\\nlocalization visual that we we just\\ncreated\\nand what we want to do is identify the\\nbounding box </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that's just kind of\\nsurrounding those values okay so we know\\nin terms of like a coordinate system we\\nwant one </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and three and four and ten to\\nbe included within that so what we do is\\nfind the corners from the detection\\narray</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or or set of\\ncoordinates that we got before from NP\\nnon-zero and what we do is we just take\\nthe minimum X and Y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">values and maximum X\\nand Y values and that will give us the\\ncorners of the box\\nthat's pretty simple to to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">calculate now\\nwhen we get the maximum value what we\\nwant to do is because we basically we\\nget in the position of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the patch and the\\nposition of each patch we're essentially\\nidentifying the top left corner of each\\npatch so when</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we're looking at the\\nmaximum value we actually want not the\\nsolved patch but the end of the patch\\nokay so that's</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">why we add that plus one\\nhere in order to get that\\nthe same for the x max value as well so\\nthat gives us a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corner coordinates and\\nthen what we do is multiply those Corner\\ncoordinates by the patch size it's 256\\npixels </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and then we have the pixel\\npositions of each one of those Corners\\nbecause before we had the patch\\ncoordinates </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">now we have the pixel\\ncoordinates which we can map directly\\nonto the original image so we can see\\nthe minimum </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">values here so we have for x\\nand y two five six and a seven six eight\\nand what we want to do because we're\\ngoing</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to be using matplotlib patches and\\nmatplotlip patches expects the top left\\ncorner coordinates and the width </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nheight of the bounding box that you want\\nto create so we calculate the width and\\nheight and that's pretty </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">simple it's\\njust y Max minus y Min and X knives\\nminus X min\\nlook at these\\nand what we can do now is take the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">image\\nwe have to\\nreshape it a little bit so we have to\\nmove the three color channels Dimension\\nfrom the zeroth </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dimension to the final\\nDimension so we just do that here move\\naxis\\nand now we can plot that image okay so\\nwe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">showed that image with matplotlib\\nand then we create a rectangle patch\\nthis is our bounding box okay so we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pass\\nX Min and Y Min that's the top left\\ncorner and then we also pass the width\\nand height of what the boundary </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">box\\nshould be\\nand if we come down we get this visual\\nokay so that's our bounding box\\nvisualization and with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that it's not\\nmuch further to create our object\\ndetection so let's have a look at how we\\ndo that now the logic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for this is pretty\\nmuch just a loop over what we've already\\ndone so I've put together a load of\\nfunctions here </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">which is essentially just\\nwhat we've already gone through getting\\npatches getting the the scores getting\\nthe the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">box and then the one thing that\\nis new here is this detect function okay\\nso we have detect that's going to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">get\\nthe the patches so it's going to take an\\nimage and it's going to split into those\\npatches that we created </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we're going to\\nconvert the image into a format for\\ndisplaying with matplotlib we did that\\nbefore and we also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">initialized that plot\\nand add our image to that plot and then\\nwhat we do is we have for Loop and this\\nfor Loop </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">goes through the image\\nlocalization steps and bounding box\\nsteps that we just went through\\njust multiple times </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">okay so we have\\nmultiple primes and we want to do it\\nmultiple times so we calculate our\\nsimilarity scores based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on a specific\\nprompt\\nfor all of our image Patches from that\\nwe get our our scores in that patch\\ntensor format </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that we saw before\\nand then what we do is we want to get\\nthe box based on a particular threshold\\nso 0.5 like we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">used before you can see\\nover there we have our patch size we\\njust need to pass to that for a\\ncalculation of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or for the conversion\\nand we have our patch size which we\\npassed to that for the conversion from\\npatch\\npixel </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">from patch coordinates to pixel\\ncoordinates now we also have our scores\\nand that will return the minimum X and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Y\\ncoordinates and also width and height of\\nthe box\\nwe create the bounding box\\nnow we add that to the axis okay </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">so now\\nlet's visualize all this see what we get\\nso here I've used a slightly smaller\\nwindow size before using </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">six just to\\npoint out that you can change it and\\ndepending on your image it may be better\\nto use a smaller or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">larger window\\nand you can see so what we're doing here\\nwe've got a cat and a butterfly and you\\ncan see that we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">get we get a butterfly\\nhere and we get the cat here okay it's\\npretty cool and like I said with clip we\\ncan apply</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this object detection without\\nfine tuning all we need to do is change\\nthese prompts here\\nokay so it's it's </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">really straightforward\\nto modify this and move it to a new\\ndomain okay so that's it for this\\nwalkthrough of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">object localization and\\nobject detection with clip as I said I\\nthink zero shot object localization\\ndetection and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">even classification opens\\nthe doors to a lot of projects and use\\ncases that were just not accessible\\nbefore </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">because time and capital\\nconstraints and now we can just use clip\\nand get pretty impressive results very\\nquickly</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">all it requires is a bit of code\\nchanging here and there now I think clip\\nis one part of a trend in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multimodality\\nthat is kind of creating a more\\naccessible ml that is less brittle like\\nmodels were in the past </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that required a\\nlot of fine tuning just to adapt to a\\nslightly different domain and just more\\ngenerally </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">applicable which I thing is\\nreally exciting and I'm I'm it's really\\ncool to see this sort of thing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">actually\\nbeing used and to actually use it and\\njust see how easy it is to use clip for\\nso many different use </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cases and it work\\nlike incredibly easily so that's it for\\nthis video I hope it has been useful\\nso thank you very</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">much for watching and\\nI will see you again in the next one bye\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">mimetype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">start_char_idx</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">end_char_idx</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">text_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{metadata_str}\\n\\n{content}'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{key}: {value}'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata_seperator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid_\u001b[0m=\u001b[32m'i3OYlaoj-BM'\u001b[0m,\n",
       "    \u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'video_id'\u001b[0m: \u001b[32m'i3OYlaoj-BM'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexcluded_embed_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mexcluded_llm_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mrelationships\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mtext\u001b[0m=\u001b[32m\"the\u001b[0m\u001b[32m imaginative laws scale visual\\nrecognition challenge was a\\nworld-changing competition\\nthat ran from\u001b[0m\n",
       "\u001b[32maround 2010 to 2017.\\nduring his time the competition acted as\\nthe place to go if you needed to find\\nwhat the \u001b[0m\n",
       "\u001b[32mcurrent state of the art was in\\nimage classification object localization\\nobject detection as well as that\\n2012 \u001b[0m\n",
       "\u001b[32monwards it really acted as the\\nCatalyst of the explosion in deep\\nlearning researchers fine-tuned \u001b[0m\n",
       "\u001b[32mbetter\\nperforming computer vision models year\\non year but there was a unquestioned\\nassumption causing \u001b[0m\n",
       "\u001b[32mproblems\\nit was assumed that every new task\\nrequired model fine tuning this required\\na lot of data and a lot of \u001b[0m\n",
       "\u001b[32mdata required\\na lot of capital and time it wasn't\\nuntil recently that this assumption was\\nchallenged and proven \u001b[0m\n",
       "\u001b[32mwrong the\\nastonishing rise of what are called\\nmulti-modal models has made the what was\\nthought impossible very \u001b[0m\n",
       "\u001b[32mpossible across\\nvarious domains and tasks one of those\\nis called zero shot object detection and\\nlocalization now\u001b[0m\n",
       "\u001b[32mzero shot refers to\\ntaking a model and applying it to a new\\ndomain without ever fine-tuning it on\\ndata from that\u001b[0m\n",
       "\u001b[32mnew domain so that means\\nwe can take a model who can it maybe it\\nworks in in one domain classification in\\none \u001b[0m\n",
       "\u001b[32mparticular area on one data set and\\nwe can take that same model without any\\nfine tuning and we can use it for \u001b[0m\n",
       "\u001b[32mobject\\ndetection in a completely different\\ndomain without that model seeing any\\ntraining data from that new \u001b[0m\n",
       "\u001b[32mdomain so in\\nthis video we're going to explore how to\\nuse open ai's clip for zero shots object\\ndetection and \u001b[0m\n",
       "\u001b[32mlocalization let's begin\\nwith taking a quick look at image\\nclassification now image classification\\ncan kind of \u001b[0m\n",
       "\u001b[32mbe seen as one of the\\nsimplest tasks in visual recognition and\\nit's also the first step on the way to\\nobject \u001b[0m\n",
       "\u001b[32mdetection at his core it's just\\nassigning a categorical label to an\\nimage now moving on from \u001b[0m\n",
       "\u001b[32mimage\\nclassification we have object\\nlocalization object localization is\\nimage classification followed by \u001b[0m\n",
       "\u001b[32mthe\\nidentification of where in the image the\\nspecific object actually is so we are\\nlocalizing the the object now\u001b[0m\n",
       "\u001b[32mdoing that\\nwhere essentially just going to identify\\nthe coordinates on the image I going to\\nreturn the typical \u001b[0m\n",
       "\u001b[32mapproach to this is\\nreturn an image where you have like a\\nbounding box surrounding the the object\\nthat you are \u001b[0m\n",
       "\u001b[32mlooking for and then we\\ntake this one step further to perform\\nobject detection with detection we are\\nlocalizing \u001b[0m\n",
       "\u001b[32mmultiple objects within the\\nimage or we have the capability to\\nidentify multiple objects within the\\nimage so in \u001b[0m\n",
       "\u001b[32mthis example we have cat and\\na dog we would expect with object\\ndetection to identify both the cat and\\nthe dot in\u001b[0m\n",
       "\u001b[32mthe case of us having\\nmultiple dolbs in this image almost Cuts\\nin this image we would also expect the\\nobject \u001b[0m\n",
       "\u001b[32mdetection algorithm to actually\\nidentify each one of those independently\\nnow in the past if we wanted to switch \u001b[0m\n",
       "\u001b[32ma\\nmodel between anyone these tasks would\\nhave to fine-tune or more data you want\\nto switch it to another domain \u001b[0m\n",
       "\u001b[32mwe would\\nhave to also fine tune it on new data\\nfrom that domain but that's not always\\nthe case with models like \u001b[0m\n",
       "\u001b[32mopen ai's clip\\nfor performing each one of these tasks\\nin a zero shot setting now open ai's\\nclip is a multi-modal\u001b[0m\n",
       "\u001b[32mmodel that has\\nbeen pre-trained on a huge number of\\ntext and image Pairs and it essentially\\nworks by identifying\u001b[0m\n",
       "\u001b[32mtext and image\\npairs that have a similar meaning and\\nplacing them within a similar Vector\\nspace so every text \u001b[0m\n",
       "\u001b[32mand every image gets\\nconverted into a vector and they are\\nplaced in a shared Vector space and the\\nvectors that \u001b[0m\n",
       "\u001b[32mappear close together they\\nhave a similar meaning now Clips very\\nbroad pre-training means that it can\\nperform \u001b[0m\n",
       "\u001b[32mvery effectively across a lot of\\ndifferent domains it's seen a lot of\\ndata and so it has a good understanding\\nof\u001b[0m\n",
       "\u001b[32mall these different things and we can\\neven adjust the task being performed\\nwith just a few code changes we \u001b[0m\n",
       "\u001b[32mdon't\\nactually have to is the model itself we\\njust adjust the code around the model\\nand that's very much thanks \u001b[0m\n",
       "\u001b[32mto Clips\\nfocus on sort of comparing these vectors\\nso for example for classification we\\ngive clip a list of our \u001b[0m\n",
       "\u001b[32mplus labels and\\nthen we pass in the images and we just\\nidentify within that space where those\\nimages are with \u001b[0m\n",
       "\u001b[32mrespect to those plus\\nlabel vectors and which plus label is\\nthe most similar to our particular image\\nand then \u001b[0m\n",
       "\u001b[32mthat is our prediction so that\\nmost similar plus label\\nthat's our predated class now for\\nobjects localization we\u001b[0m\n",
       "\u001b[32mapply a very\\nsimilar type of logic as before we\\ncreate a class label but unlike before\\nwe don't need the entire \u001b[0m\n",
       "\u001b[32mimage into clip\\nto localize an object we have to break\\nthe image into patches we then pass a\\nwindow over all of \u001b[0m\n",
       "\u001b[32mthose patches moving\\nacross the entire image left to right\\ntop to bottom and we generate a image\\nembedding for \u001b[0m\n",
       "\u001b[32meach of those windows and\\nthen we calculate the similarity between\\neach one of those windows embedded by\\nclip \u001b[0m\n",
       "\u001b[32mand the class label embedding\\nreturning a similarity score for every\\nsingle patch now after calculating \u001b[0m\n",
       "\u001b[32mthe\\nsimilarity score for every single patch\\nwe use that to create almost like a map\\nof relevance across the \u001b[0m\n",
       "\u001b[32mentire image and\\nthen we can use that map to identify the\\nlocation of the object of interest and\\nfrom that we \u001b[0m\n",
       "\u001b[32mwill get something that's\\nkind of like this so we have most of the\\nimage will be very dark and black that\\nmeans \u001b[0m\n",
       "\u001b[32mas the object of interest is not\\nin that space and then using that\\nlocalization map we can create a \u001b[0m\n",
       "\u001b[32mmore\\ntraditional bounding box visualization\\nas well both of these visuals are\\ncapturing the same information \u001b[0m\n",
       "\u001b[32mwe're\\njust displaying it in a different way\\nnow there's also other approaches to\\nthis so I recently hosted a \u001b[0m\n",
       "\u001b[32mtalk with\\nwhat two two sets of people actually so\\nFederico Bianchi from Stanford's NLP\\ngroup and also Raphael \u001b[0m\n",
       "\u001b[32mpissoni and both\\nof those have worked on a Italian clip\\nproject and part of that was performing\\nobject \u001b[0m\n",
       "\u001b[32mlocalization now to do that they\\nuse a slightly different approach what\\nI'm going to demonstrate here and we \u001b[0m\n",
       "\u001b[32mcan\\nthink of it as almost like the opposite\\nso whereas we slide a window over the\\nwhole image they slide a black\u001b[0m\n",
       "\u001b[32mpatch\\nover the whole image which hides what is\\nbehind in that patch and then they feed\\nthe image into click and \u001b[0m\n",
       "\u001b[32messentially as\\nyou slide the patch over the image you\\nare hiding a part of the image and\\ntherefore if this \u001b[0m\n",
       "\u001b[32msimilarity score drops\\nwhen the patch is over a certain area\\nyou know that the object you're looking\\nfor is \u001b[0m\n",
       "\u001b[32mprobably within that space and\\nthat's called the occlusion algorithm\\nnow moving on to object detection which\\nis \u001b[0m\n",
       "\u001b[32mlike the last level in these three\\ntasks we will be identifying multiple\\nobjects now there's a very fine \u001b[0m\n",
       "\u001b[32mline\\nbetween object localization and object\\ndetection but you can simply think of it\\nas localization for \u001b[0m\n",
       "\u001b[32mmultiple clusters\\nand multiple objects with our cat and\\nButterfly image we will be searching for\\ntwo objects a \u001b[0m\n",
       "\u001b[32mcat and a butterfly and\\nwith that we could draw a bounding box\\naround both of those objects and\\nessentially what\u001b[0m\n",
       "\u001b[32mwe're doing now is\\nusing localization for a single object\\nbut then we're putting both of those\\ntogether in a \u001b[0m\n",
       "\u001b[32mloop in our code and we're\\nproducing this object detection process\\nnow we've covered the idea behind my\\nimage \u001b[0m\n",
       "\u001b[32mclassification onto object\\nlocalization and object detection now\\nlet's have a look at how we actually\\nImplement \u001b[0m\n",
       "\u001b[32mall of this now before we move\\non to any classification localization or\\ndetection task we need to have some \u001b[0m\n",
       "\u001b[32mdata\\nwe're going to use a small demo data set\\ncalled James Callum image text demo and\\nwe can download it like \u001b[0m\n",
       "\u001b[32mthis so we're\\nusing hooking phase data sets here which\\nwe can pip install with Pip install\\ndata sets\\nand this \u001b[0m\n",
       "\u001b[32mis the day so it's very small\\nit's 21 text to image pairs okay one of\\nthose is the image you've already seen\\nthe\u001b[0m\n",
       "\u001b[32mcat with a butterfly landing on its\\nnose very curious how they got that\\nphoto now after you've downloaded \u001b[0m\n",
       "\u001b[32mthat\\ndata set\\nwe're going to be using this image here\\nand what we want to do is not use the\\nimage file itself \u001b[0m\n",
       "\u001b[32mbecause at the moment\\nit's a it's a pill python image object\\nbut instead we need to convert it into a\\ncanister \u001b[0m\n",
       "\u001b[32mnow we're going to be using pi\\ntorch later on so what I want to do here\\nis we're going to just transform \u001b[0m\n",
       "\u001b[32mthe\\nimage into a tensor and we use toxin\\ntransforms use the typical pipeline tool\\nin computer vision and we just\u001b[0m\n",
       "\u001b[32muse tube\\ntensor okay and then we process our\\nimage through that Pipeline and then we\\ncan see that we get this \u001b[0m\n",
       "\u001b[32mokay so what\\nare these values here we have the height\\nof the image in pixels\\nthe width of the image in pixels \u001b[0m\n",
       "\u001b[32mand\\nthen also the three color channels red\\ngreen and blue that make up the image\\nnow we need a slightly \u001b[0m\n",
       "\u001b[32mdifferent format\\nwhen we are processing everything one we\\nneed to add those patches and two we\\nneed to process \u001b[0m\n",
       "\u001b[32mit through a pie torch\\nmodel and we also need the batch\\ndimension for that so the first thing\\nwe're going to do \u001b[0m\n",
       "\u001b[32mis add the batch\\nDimension it's just a single image so we\\njust have one in there but we we need\\nthat anyway and \u001b[0m\n",
       "\u001b[32mthen we come down to\\nhere so this is where we're going to\\nbreak the image into the patches okay\\neach patch is \u001b[0m\n",
       "\u001b[32mgoing to be 256 dimensions\\nin both height and width so the first\\nthing we do here is unfold and we get\\nthis here\u001b[0m\n",
       "\u001b[32mokay there's two five six and\\nthere's 20. now the 20 is the height of\\nthe image in these 256 pixel patches\\nand \u001b[0m\n",
       "\u001b[32mwe can visualize that here\\nall right so now we have all these kind\\nof like slivers of the image that's just\\na \u001b[0m\n",
       "\u001b[32mvertical component of each patch\\nand we use unfold again but in this time\\nin the second Dimensions the \u001b[0m\n",
       "\u001b[32mtargeting\\nwhat was this Dimension here and we also\\nget another 256 now we visualize that we\\nget our four \u001b[0m\n",
       "\u001b[32mpatches\\nokay like this\\nnow if you just consider this here it's\\nlike if we look at this patch here it\\ndoesn't \u001b[0m\n",
       "\u001b[32mtell us anything about the image\\nright and even when we're over cats\\nthese patches are way too small to\\nactually\u001b[0m\n",
       "\u001b[32mtell us anything if clip is\\nprocessing a single patch at a time\\nit's probably not going to tell us\\nanything \u001b[0m\n",
       "\u001b[32mmaybe it could tell us that\\nthere's some hair in this patch or that\\nthere's an eye in this patch but beyond\\nthat\u001b[0m\n",
       "\u001b[32mit's not going to be very useful so\\nrather than feeding single patches into\\nclip what we do is actually feed \u001b[0m\n",
       "\u001b[32ma\\nwindow a six by six patches or we can\\nmodify that value if we prefer and that\\njust gives us a big patch to \u001b[0m\n",
       "\u001b[32mpass over\\nto clip now the reason that we don't\\njust do that from the start we don't\\njust create these bigger \u001b[0m\n",
       "\u001b[32mpatches to\\nbegin with is because when we're sliding\\nthrough the image we want to have some\\ndegree of overlap \u001b[0m\n",
       "\u001b[32mbetween each patch\\nokay so we create these smaller patches\\nand then what we can do is actually\\nslide across just\u001b[0m\n",
       "\u001b[32mone little patch at a\\ntime and we Define that using the stride\\nvariable so if we come down to here\\nwe have \u001b[0m\n",
       "\u001b[32mwindow we have stride\\nremove this\\nand here we go this is our code for\\ngoing through the whole image creating \u001b[0m\n",
       "\u001b[32ma\\npatch at every time step okay so we go\\nfor y and we go through the whole y-axis\\nand then within that we're \u001b[0m\n",
       "\u001b[32mgoing across\\nleft to right with each step and we\\ninitialize a empty big patch array so\\nthis is our like the full\u001b[0m\n",
       "\u001b[32mwindow\\nwe got the current batch so okay let's\\nsay we start at zero zero X zero y zero\\nwe go from zero to six and\u001b[0m\n",
       "\u001b[32mzero to six\\nhere\\nright so that gives us the very top left\\ncorner or window of the image and then\\nwe're \u001b[0m\n",
       "\u001b[32mliterally going through and and\\njust go processing all of that and you\\ncan see that happening here as wine \u001b[0m\n",
       "\u001b[32meggs\\nare increasing we're moving through that\\nimage and we're seeing each big patch\\nfrom our image okay sliding \u001b[0m\n",
       "\u001b[32macross with\\na single small little patch at a time so\\nthat we don't miss any important\\ninformation\\nnow this is \u001b[0m\n",
       "\u001b[32mhow we're going to run\\nthrough the whole image but before we do\\nthat we actually need clip so let's go\\nahead and\u001b[0m\n",
       "\u001b[32mactually initialize clip\\nso to do that all we do is this so we're\\nusing hook and face Transformers which\\nis \u001b[0m\n",
       "\u001b[32musing pi torch in the in the back\\nthere so we need the clip processor\\nwhich is like a pre-processing \u001b[0m\n",
       "\u001b[32mpipeline\\nfor both text and images and then the\\nactual model itself okay so we some\\nmodel ID and we initialize \u001b[0m\n",
       "\u001b[32mboth of those\\nthen what we want to do is move the\\nmodel to advice if possible all right so\\nwe can use CPU but if\u001b[0m\n",
       "\u001b[32myou have a Kudo\\nenabled GPU that will be here much\\nfaster so I'd recommend doing that if\\nyou can if not then you\u001b[0m\n",
       "\u001b[32mcan use CPU it\\nwill be a bit slower but it will still\\nrun within a variable time frame so if\\nI'm running this on\u001b[0m\n",
       "\u001b[32mmy Mac I am using a\\nCPU you can actually run this on MPS as\\nwell so you could change your device to\\nMPS if you \u001b[0m\n",
       "\u001b[32mhave an MPS enabled Apple\\nsilicon device\\nso now returning to that process where\\nwe're going through each window \u001b[0m\n",
       "\u001b[32mwithin\\nthe image we're just going to add a\\nlittle bit more logic so we are\\nprocessing like we were before \u001b[0m\n",
       "\u001b[32mthere's\\nnothing different here we're creating\\nthat big patch and then what we do is\\nprocess that big patch and \u001b[0m\n",
       "\u001b[32mprocess a\\ntext label okay so at the moment we're\\nlooking for a fluffy cat within this\\nimage so that is how we do\u001b[0m\n",
       "\u001b[32mthis we're\\nreturning Pi torch tensors we also add\\npadding here as well for the text\\nalthough in this case I \u001b[0m\n",
       "\u001b[32mdon't think we\\nneed it because we only have a single\\ntext item but we include that when we're\\nusing multiple \u001b[0m\n",
       "\u001b[32mtext items later and then\\nwe calculate and retrieve the similarity\\nscore between them okay so if we pass\\nboth \u001b[0m\n",
       "\u001b[32mtext and images through this\\nprocessor we'll pass both into our\\ninputs here and then we just calculate\\nthe or we\u001b[0m\n",
       "\u001b[32mextract the logits for each\\nimage and the item just converts that\\ninto a array of values or single value\\nand \u001b[0m\n",
       "\u001b[32mthen here we have those scores so\\nwhat we're doing here is creating the\\nwhat I earlier called like the \u001b[0m\n",
       "\u001b[32mrelevance\\nmap or localization map throughout the\\nwhole image so for every window that we\\ngo through we're adding\u001b[0m\n",
       "\u001b[32mthis score to\\nevery single patch or little patch\\nwithin that window and what we're going\\nto do or what we're \u001b[0m\n",
       "\u001b[32mgoing to find when\\nwe do that is that some patches will\\nnaturally have a high score than others\\nbecause they are\u001b[0m\n",
       "\u001b[32mviewed more times right\\nso if you think about the top left patch\\nin the image that's only going to be\\nviewed \u001b[0m\n",
       "\u001b[32monce whereas patches in the\\nmiddle are going to be viewed many times\\nbecause we'll have a sliding window\\ngoing \u001b[0m\n",
       "\u001b[32mover there multiple times so what\\nwe also need to do is identify the\\nnumber of runs that we perform or number\\nof\u001b[0m\n",
       "\u001b[32mcalculations that we perform within\\neach one of those patches the reason we\\ndo that is so that we can take \u001b[0m\n",
       "\u001b[32mthe\\naverage for each score based on the\\nnumber of times that score has been\\ncapital related because here \u001b[0m\n",
       "\u001b[32mwe're\\ntaking the total Awards scores and then\\nwe'll just take the average like so now\\nthe scores tensor is going\u001b[0m\n",
       "\u001b[32mto have a\\nvery smooth gradient of values from zero\\ncompletely irrelevant to one now if you\\nconsider that we've \u001b[0m\n",
       "\u001b[32mbeen going over\\nthese scores multiple times it means\\nthat the object of interest is kind of\\nlike faded out of \u001b[0m\n",
       "\u001b[32mthe window like over\\nmultiple steps so that means that the\\nsimilarities score quite gradually faves\\nout as you \u001b[0m\n",
       "\u001b[32mgo away from the object which\\nmeans that you don't really get very\\ngood localization if you use these\\nscores \u001b[0m\n",
       "\u001b[32mdirectly so what you what we need\\nto do is actually clip the lower scores\\ndown to zero so to do that what it \u001b[0m\n",
       "\u001b[32mis\\ncalculate the average of scores across\\nthe whole image we subtract that average\\nfrom the current scores what \u001b[0m\n",
       "\u001b[32mthat will\\ndo is push 50 of the scores below zero\\nand then we click those scores so\\nanything below zero becomes \u001b[0m\n",
       "\u001b[32mzero and we\\ncan do this multiple times okay one time\\nis usually enough but you can do it\\nmultiple times to \u001b[0m\n",
       "\u001b[32mincrease that effect\\nof making the edge of this detected or\\nlocalized area better defined and then\\nafter you've \u001b[0m\n",
       "\u001b[32mdone that what we need to\\ndo is normalize those scores okay so we\\nmight have to do this a few times \u001b[0m\n",
       "\u001b[32mbut\\neverything's probably going to be within\\nthe range of like zero to 0.5 or 0 to\\n0.2 so then we normalize \u001b[0m\n",
       "\u001b[32mthose scores\\nbring them back within the range of zero\\nto one now to apply these scores to the\\npatches we need to\u001b[0m\n",
       "\u001b[32malign their tenses\\nbecause right now that they are not\\naligned okay for the scores we just we\\nhave like 20 by 13\u001b[0m\n",
       "\u001b[32mtensor but for the\\npatches we have the the batch Dimension\\nthere we have the 20 by 13 which we do\\nwant but then \u001b[0m\n",
       "\u001b[32mwe have the three color\\nchannels and the two five six for each\\num set of pixels within each patch so we\\nneed to \u001b[0m\n",
       "\u001b[32madjust that a little bit so we\\nneed to First remove the batch Dimension\\nwe do that by squeezing out the \u001b[0m\n",
       "\u001b[32mzero\\nDimension which is a batch Dimension and\\nthen we permute the different dimensions\\nit's actually just moving\u001b[0m\n",
       "\u001b[32mthem around in\\nour patches in order to align them\\nbetter with the score tensor dimensions\\nand then all we do is \u001b[0m\n",
       "\u001b[32mmultiply the\\npatches by those scores that's pretty\\nstraightforward\\nthen we have to mute them again because\\nif \u001b[0m\n",
       "\u001b[32mwe want to visualize everything needs\\nto be within a certain shape in order\\nfor us to visualize our \u001b[0m\n",
       "\u001b[32mmatplotlib\\nso we come down and the first thing you\\ndo is just get y next here so Y and X\\nare the the \u001b[0m\n",
       "\u001b[32mpatches\\nsee here this is y so the height of the\\nimage in patches and then 13 which is\\nthe width of the image in \u001b[0m\n",
       "\u001b[32mpatches and we\\ncome down here and we can plot this okay\\nand we get this it's pretty nice visual\\nlocalizes the \u001b[0m\n",
       "\u001b[32mThe Fluffy Cuts within\\nthat image now what's really interesting\\nis if we just search for a cat we\\nactually get a\u001b[0m\n",
       "\u001b[32mslightly different\\nlocalization because here you can see\\nit's kind of focusing a lot on the\\nfluffy part of the \u001b[0m\n",
       "\u001b[32mcat so if we just\\nsearch for a cat it would actually focus\\nmore on the head so we can really add\\nnuanced \u001b[0m\n",
       "\u001b[32minformation to these prompts and\\nget a pretty nuanced response back\\nnow we can do the same for butterfly \u001b[0m\n",
       "\u001b[32mso\\nwe'll just throw all that code together\\nthis is just what we've done before we\\ninitialize scores and runs and\u001b[0m\n",
       "\u001b[32mwe go\\nprocess all of that the only thing we\\nchange here is the prompt we change it\\nto a butterfly and if we go \u001b[0m\n",
       "\u001b[32mdown and\\nwe're going to go down and down and\\nvisualize that we'll get this okay so\\nagain that's that's pretty \u001b[0m\n",
       "\u001b[32mcool we can\\nsee that it is identifying where in the\\nimage that butterfly actually is so that\\nis the object \u001b[0m\n",
       "\u001b[32mlocalization set\\nnow I want to have a look at object\\ndetection which is essentially just\\ntaking the object \u001b[0m\n",
       "\u001b[32mlocalization and and\\nwrapping some more code around it\\nin order to look at these multiple\\nobjects rather than \u001b[0m\n",
       "\u001b[32mjust one but to do\\nthat we can't really visualize in the\\nsame way that we've done here we're\\ngoing to need a \u001b[0m\n",
       "\u001b[32mdifferent type of\\nvisualization and that's where we have\\nthe bounding boxes so let's take a look\\nat how we would\u001b[0m\n",
       "\u001b[32mdo that so using the I\\nthink the butterfly examples of\\nbutterfly scores that we just calculated\\nwe're going to \u001b[0m\n",
       "\u001b[32mlook at where those\\nscores are higher than 0.5 now you can\\nadjust this threshold based on you know\\nwhat you find\u001b[0m\n",
       "\u001b[32mworks best so we do this\\nand what we'll get is a array of true\\nand false values as to where the score\\nwas higher\u001b[0m\n",
       "\u001b[32mthan 0.5 and not\\nand then we detect where the non-zero\\nvalues are in that array and what we do\\nis get a load of \u001b[0m\n",
       "\u001b[32mX and Y values here\\nso position three two we know that there\\nis a score that is higher than 0.5 and\\nwe get three\u001b[0m\n",
       "\u001b[32mand two here so three is\\nthe row of the non-zero value and 2 is a\\ncolumn of the non-zero value so at \u001b[0m\n",
       "\u001b[32mrow\\nposition three and column two we know\\nthat there is a non-zero value or a\\nvalue or score that's higher than \u001b[0m\n",
       "\u001b[32m0.5\\nour threshold\\nand put all that together we'll get\\nsomething looks kind of like this so we\\nalready we kind \u001b[0m\n",
       "\u001b[32mof see that\\nlocalization visual that we we just\\ncreated\\nand what we want to do is identify the\\nbounding box \u001b[0m\n",
       "\u001b[32mthat's just kind of\\nsurrounding those values okay so we know\\nin terms of like a coordinate system we\\nwant one \u001b[0m\n",
       "\u001b[32mand three and four and ten to\\nbe included within that so what we do is\\nfind the corners from the detection\\narray\u001b[0m\n",
       "\u001b[32mor or set of\\ncoordinates that we got before from NP\\nnon-zero and what we do is we just take\\nthe minimum X and Y \u001b[0m\n",
       "\u001b[32mvalues and maximum X\\nand Y values and that will give us the\\ncorners of the box\\nthat's pretty simple to to \u001b[0m\n",
       "\u001b[32mcalculate now\\nwhen we get the maximum value what we\\nwant to do is because we basically we\\nget in the position of\u001b[0m\n",
       "\u001b[32mthe patch and the\\nposition of each patch we're essentially\\nidentifying the top left corner of each\\npatch so when\u001b[0m\n",
       "\u001b[32mwe're looking at the\\nmaximum value we actually want not the\\nsolved patch but the end of the patch\\nokay so that's\u001b[0m\n",
       "\u001b[32mwhy we add that plus one\\nhere in order to get that\\nthe same for the x max value as well so\\nthat gives us a \u001b[0m\n",
       "\u001b[32mcorner coordinates and\\nthen what we do is multiply those Corner\\ncoordinates by the patch size it's 256\\npixels \u001b[0m\n",
       "\u001b[32mand then we have the pixel\\npositions of each one of those Corners\\nbecause before we had the patch\\ncoordinates \u001b[0m\n",
       "\u001b[32mnow we have the pixel\\ncoordinates which we can map directly\\nonto the original image so we can see\\nthe minimum \u001b[0m\n",
       "\u001b[32mvalues here so we have for x\\nand y two five six and a seven six eight\\nand what we want to do because we're\\ngoing\u001b[0m\n",
       "\u001b[32mto be using matplotlib patches and\\nmatplotlip patches expects the top left\\ncorner coordinates and the width \u001b[0m\n",
       "\u001b[32mand\\nheight of the bounding box that you want\\nto create so we calculate the width and\\nheight and that's pretty \u001b[0m\n",
       "\u001b[32msimple it's\\njust y Max minus y Min and X knives\\nminus X min\\nlook at these\\nand what we can do now is take the \u001b[0m\n",
       "\u001b[32mimage\\nwe have to\\nreshape it a little bit so we have to\\nmove the three color channels Dimension\\nfrom the zeroth \u001b[0m\n",
       "\u001b[32mdimension to the final\\nDimension so we just do that here move\\naxis\\nand now we can plot that image okay so\\nwe \u001b[0m\n",
       "\u001b[32mshowed that image with matplotlib\\nand then we create a rectangle patch\\nthis is our bounding box okay so we \u001b[0m\n",
       "\u001b[32mpass\\nX Min and Y Min that's the top left\\ncorner and then we also pass the width\\nand height of what the boundary \u001b[0m\n",
       "\u001b[32mbox\\nshould be\\nand if we come down we get this visual\\nokay so that's our bounding box\\nvisualization and with \u001b[0m\n",
       "\u001b[32mthat it's not\\nmuch further to create our object\\ndetection so let's have a look at how we\\ndo that now the logic \u001b[0m\n",
       "\u001b[32mfor this is pretty\\nmuch just a loop over what we've already\\ndone so I've put together a load of\\nfunctions here \u001b[0m\n",
       "\u001b[32mwhich is essentially just\\nwhat we've already gone through getting\\npatches getting the the scores getting\\nthe the\u001b[0m\n",
       "\u001b[32mbox and then the one thing that\\nis new here is this detect function okay\\nso we have detect that's going to \u001b[0m\n",
       "\u001b[32mget\\nthe the patches so it's going to take an\\nimage and it's going to split into those\\npatches that we created \u001b[0m\n",
       "\u001b[32mwe're going to\\nconvert the image into a format for\\ndisplaying with matplotlib we did that\\nbefore and we also \u001b[0m\n",
       "\u001b[32minitialized that plot\\nand add our image to that plot and then\\nwhat we do is we have for Loop and this\\nfor Loop \u001b[0m\n",
       "\u001b[32mgoes through the image\\nlocalization steps and bounding box\\nsteps that we just went through\\njust multiple times \u001b[0m\n",
       "\u001b[32mokay so we have\\nmultiple primes and we want to do it\\nmultiple times so we calculate our\\nsimilarity scores based \u001b[0m\n",
       "\u001b[32mon a specific\\nprompt\\nfor all of our image Patches from that\\nwe get our our scores in that patch\\ntensor format \u001b[0m\n",
       "\u001b[32mthat we saw before\\nand then what we do is we want to get\\nthe box based on a particular threshold\\nso 0.5 like we \u001b[0m\n",
       "\u001b[32mused before you can see\\nover there we have our patch size we\\njust need to pass to that for a\\ncalculation of the \u001b[0m\n",
       "\u001b[32mor for the conversion\\nand we have our patch size which we\\npassed to that for the conversion from\\npatch\\npixel \u001b[0m\n",
       "\u001b[32mfrom patch coordinates to pixel\\ncoordinates now we also have our scores\\nand that will return the minimum X and \u001b[0m\n",
       "\u001b[32mY\\ncoordinates and also width and height of\\nthe box\\nwe create the bounding box\\nnow we add that to the axis okay \u001b[0m\n",
       "\u001b[32mso now\\nlet's visualize all this see what we get\\nso here I've used a slightly smaller\\nwindow size before using \u001b[0m\n",
       "\u001b[32msix just to\\npoint out that you can change it and\\ndepending on your image it may be better\\nto use a smaller or \u001b[0m\n",
       "\u001b[32mlarger window\\nand you can see so what we're doing here\\nwe've got a cat and a butterfly and you\\ncan see that we \u001b[0m\n",
       "\u001b[32mget we get a butterfly\\nhere and we get the cat here okay it's\\npretty cool and like I said with clip we\\ncan apply\u001b[0m\n",
       "\u001b[32mthis object detection without\\nfine tuning all we need to do is change\\nthese prompts here\\nokay so it's it's \u001b[0m\n",
       "\u001b[32mreally straightforward\\nto modify this and move it to a new\\ndomain okay so that's it for this\\nwalkthrough of \u001b[0m\n",
       "\u001b[32mobject localization and\\nobject detection with clip as I said I\\nthink zero shot object localization\\ndetection and\u001b[0m\n",
       "\u001b[32meven classification opens\\nthe doors to a lot of projects and use\\ncases that were just not accessible\\nbefore \u001b[0m\n",
       "\u001b[32mbecause time and capital\\nconstraints and now we can just use clip\\nand get pretty impressive results very\\nquickly\u001b[0m\n",
       "\u001b[32mall it requires is a bit of code\\nchanging here and there now I think clip\\nis one part of a trend in \u001b[0m\n",
       "\u001b[32mmultimodality\\nthat is kind of creating a more\\naccessible ml that is less brittle like\\nmodels were in the past \u001b[0m\n",
       "\u001b[32mthat required a\\nlot of fine tuning just to adapt to a\\nslightly different domain and just more\\ngenerally \u001b[0m\n",
       "\u001b[32mapplicable which I thing is\\nreally exciting and I'm I'm it's really\\ncool to see this sort of thing \u001b[0m\n",
       "\u001b[32mactually\\nbeing used and to actually use it and\\njust see how easy it is to use clip for\\nso many different use \u001b[0m\n",
       "\u001b[32mcases and it work\\nlike incredibly easily so that's it for\\nthis video I hope it has been useful\\nso thank you very\u001b[0m\n",
       "\u001b[32mmuch for watching and\\nI will see you again in the next one bye\"\u001b[0m,\n",
       "    \u001b[33mmimetype\u001b[0m=\u001b[32m'text/plain'\u001b[0m,\n",
       "    \u001b[33mstart_char_idx\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mend_char_idx\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mtext_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmetadata_str\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[33mmetadata_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mkey\u001b[0m\u001b[32m}\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mvalue\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[33mmetadata_seperator\u001b[0m=\u001b[32m'\\n'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print (documents[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20ec944a-3e58-4444-a181-a99d21f117c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.schema.Document"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
